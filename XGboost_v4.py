# åœ¨ xgboost_v4.py æ–‡ä»¶çš„æœ€å¼€å¤´ï¼ˆimportä¹‹å‰ï¼‰æ·»åŠ ä»¥ä¸‹ä»£ç ï¼š

import os
import warnings
import sys

# æŠ‘åˆ¶æ‰€æœ‰FutureWarningå’ŒUserWarning
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning)
warnings.filterwarnings('ignore', category=DeprecationWarning)

# æŠ‘åˆ¶XGBoostçš„ç³»ç»Ÿè­¦å‘Š
os.environ['PYTHONWARNINGS'] = 'ignore'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # å¦‚æžœæœ‰tensorflowç›¸å…³è­¦å‘Š

# é‡å®šå‘stderrä»¥å‡å°‘è¾“å‡º
class SuppressOutput:
    def __enter__(self):
        self._original_stderr = sys.stderr
        sys.stderr = open(os.devnull, 'w')
        
    def __exit__(self, exc_type, exc_val, exc_tb):
        sys.stderr.close()
        sys.stderr = self._original_stderr

import os
import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV
from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score,
                             roc_auc_score, average_precision_score)
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest, f_classif, RFE
import joblib
from skops.io import dump
from tqdm.auto import tqdm
import warnings
from scipy.stats import uniform, randint
import traceback

warnings.filterwarnings('ignore')


class BankingXGBoostV4:
    def __init__(self, top_k_features=200, n_splits=5, test_size=0.2,
                 pre_rfe_features=200, nested_cv=False, verbose=True):
        """
        XGBoost V4 - ç»ˆæžä¼˜åŒ–ç‰ˆ
        
        Parameters:
        - top_k_features: æœ€ç»ˆé€‰æ‹©çš„ç‰¹å¾æ•°é‡
        - n_splits: æ—¶åºäº¤å‰éªŒè¯æŠ˜æ•°
        - test_size: æœ€ç»ˆæµ‹è¯•é›†æ¯”ä¾‹
        - pre_rfe_features: RFEå‰çš„é¢„ç­›é€‰ç‰¹å¾æ•°
        - nested_cv: æ˜¯å¦ä½¿ç”¨åµŒå¥—äº¤å‰éªŒè¯
        - verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†è¿›åº¦
        """
        self.features = None
        self.targets = None
        self.top_k_features = top_k_features
        self.n_splits = n_splits
        self.test_size = test_size
        self.pre_rfe_features = pre_rfe_features
        self.nested_cv = nested_cv
        self.verbose = verbose
        
        self.banking_stocks = ['AXP', 'BAC', 'BK', 'C', 'CB', 'COF', 'GS',
                               'JPM', 'MS', 'PNC', 'SCHW', 'STT', 'TFC', 'USB', 'WFC']
        self.models = {}
        self.scalers = {}
        self.feature_selectors = {}
        self.results = {}
        
        # ä¸¤é˜¶æ®µè¶…å‚æ•°æœç´¢
        self.coarse_param_grid = {
            'n_estimators': [50, 100, 150],
            'max_depth': [3, 5, 7],
            'learning_rate': [0.01, 0.05, 0.1],
            'subsample': [0.8, 1.0],
            'colsample_bytree': [0.8, 1.0],
            'reg_alpha': [0, 0.1],
            'reg_lambda': [1],
            'min_child_weight': [1]
        }
        
        self.fine_param_grid = {}  # åŠ¨æ€ç”Ÿæˆ

    def log(self, message, level="INFO"):
        """æ—¥å¿—è¾“å‡º"""
        if self.verbose:
            print(f"[{level}] {message}")

    def load_data(self, feature_path='banking_returns.csv', target_path='banking_targets_ai.csv'):
        self.log("=" * 80)
        self.log("ðŸš€ XGBoost V4 - ç»ˆæžä¼˜åŒ–ç‰ˆ")
        self.log("=" * 80)
        self.log("ðŸ“Š 1. åŠ è½½æ•°æ®...")
        
        self.features = pd.read_csv(feature_path, index_col='Date', parse_dates=True)
        self.targets = pd.read_csv(target_path, index_col='Date', parse_dates=True)
        
        self.log(f"   âœ… ç‰¹å¾æ•°æ®: {self.features.shape}")
        self.log(f"   âœ… ç›®æ ‡æ•°æ®: {self.targets.shape}")
        self.log(f"   âœ… æ—¶é—´èŒƒå›´: {self.features.index[0]} åˆ° {self.features.index[-1]}")
        self.log(f"   âœ… é…ç½®: Top-{self.top_k_features}ç‰¹å¾, {self.n_splits}æŠ˜CV, " +
                 f"é¢„ç­›é€‰{self.pre_rfe_features}ç‰¹å¾, åµŒå¥—CV={'å¼€å¯' if self.nested_cv else 'å…³é—­'}")
        return True

    def prepare_data_no_leakage(self, stock, task_type='direction', horizon='1D'):
        """æ— æ•°æ®æ³„æ¼çš„æ•°æ®å‡†å¤‡"""
        target_col = f'{stock}_Direction_{horizon}'
        if target_col not in self.targets.columns:
            self.log(f"   âš ï¸ ç›®æ ‡å˜é‡ {target_col} ä¸å­˜åœ¨!", "WARNING")
            return None, None
            
        X = self.features.dropna()
        y = self.targets[target_col].dropna()
        valid_idx = X.index.intersection(y.index)
        X, y = X.loc[valid_idx], y.loc[valid_idx]
        
        self.log(f"   ðŸ“ˆ {stock}: {X.shape[0]} æ ·æœ¬, {X.shape[1]} ç‰¹å¾")
        self.log(f"   ðŸ“Š ç±»åˆ«åˆ†å¸ƒ: {dict(y.value_counts().sort_index())}")
        
        return X, y

    def hierarchical_feature_selection(self, X_train, y_train, stock):
        """
        åˆ†å±‚ç‰¹å¾é€‰æ‹©: SelectKBest -> XGB Importance -> RFE
        """
        n_samples = len(X_train)
        original_features = X_train.shape[1]
        
        self.log(f"   ðŸ” å¼€å§‹åˆ†å±‚ç‰¹å¾é€‰æ‹© (åŽŸå§‹ç‰¹å¾: {original_features})")
        
        # ç¬¬ä¸€å±‚: ç»Ÿè®¡ç­›é€‰ (å¿«é€Ÿ)
        if original_features > self.pre_rfe_features:
            selector_stat = SelectKBest(score_func=f_classif, k=self.pre_rfe_features)
            X_stat = selector_stat.fit_transform(X_train, y_train)
            selected_features_stat = X_train.columns[selector_stat.get_support()].tolist()
            self.log(f"   ðŸ“‰ ç»Ÿè®¡ç­›é€‰: {original_features} -> {len(selected_features_stat)}")
        else:
            X_stat = X_train
            selected_features_stat = X_train.columns.tolist()
            self.log(f"   ðŸ“‰ è·³è¿‡ç»Ÿè®¡ç­›é€‰ (ç‰¹å¾æ•°å·²å°‘äºŽé˜ˆå€¼)")
        
        # ç¬¬äºŒå±‚: XGBoosté‡è¦æ€§
        xgb_selector = xgb.XGBClassifier(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            random_state=42,
            n_jobs=4,
            eval_metric='logloss',
            verbosity=0
        )
        xgb_selector.fit(X_stat, y_train)
        
        # é€‰æ‹©é‡è¦æ€§æœ€é«˜çš„ç‰¹å¾
        importances = pd.Series(xgb_selector.feature_importances_, index=selected_features_stat)
        xgb_top_features = importances.nlargest(min(self.top_k_features * 2, len(selected_features_stat))).index.tolist()
        self.log(f"   ðŸš€ XGBé‡è¦æ€§ç­›é€‰: {len(selected_features_stat)} -> {len(xgb_top_features)}")
        
        # ç¬¬ä¸‰å±‚: RFE (ä»…åœ¨åˆç†æ ·æœ¬æ•°ä¸‹ä½¿ç”¨)
        if n_samples >= 500 and len(xgb_top_features) > self.top_k_features:
            self.log(f"   ðŸ”„ æ‰§è¡ŒRFE (æ ·æœ¬æ•°å……è¶³: {n_samples})")
            X_xgb = X_train[xgb_top_features]
            
            rfe = RFE(
                estimator=xgb.XGBClassifier(
                    n_estimators=50,
                    max_depth=6,
                    learning_rate=0.1,
                    random_state=42,
                    n_jobs=4,
                    eval_metric='logloss',
                    verbosity=0
                ),
                n_features_to_select=self.top_k_features,
                step=0.1
            )
            
            with tqdm(desc="RFEè¿›åº¦", disable=not self.verbose) as pbar:
                rfe.fit(X_xgb, y_train)
                pbar.update(1)
            
            final_features = X_xgb.columns[rfe.support_].tolist()
            self.log(f"   ðŸŽ¯ RFEç­›é€‰: {len(xgb_top_features)} -> {len(final_features)}")
        else:
            if n_samples < 500:
                self.log(f"   âš ï¸ RFEè·³è¿‡ (æ ·æœ¬æ•°ä¸è¶³: {n_samples})")
            elif len(xgb_top_features) <= self.top_k_features:
                self.log(f"   âš ï¸ RFEè·³è¿‡ (ç‰¹å¾æ•°å·²è¾¾æ ‡: {len(xgb_top_features)})")
            final_features = xgb_top_features[:self.top_k_features]
        
        self.log(f"   âœ… æœ€ç»ˆé€‰æ‹©ç‰¹å¾: {len(final_features)}")
        return final_features

    def nested_cross_validation(self, X_train, y_train):
        """
        åµŒå¥—äº¤å‰éªŒè¯: å¤–å±‚è¯„ä¼°æ¨¡åž‹æ€§èƒ½ï¼Œå†…å±‚è°ƒä¼˜è¶…å‚æ•°
        """
        outer_scores = []
        best_params_list = []
        
        # å¤–å±‚æ—¶åºåˆ†å‰²
        outer_cv = TimeSeriesSplit(n_splits=5)
        
        self.log(f"   ðŸ”„ åµŒå¥—CV: {self.n_splits}æŠ˜å¤–å±‚éªŒè¯")
        
        for fold_idx, (train_idx, val_idx) in enumerate(outer_cv.split(X_train)):
            if self.verbose:
                print(f"      æŠ˜æ•° {fold_idx + 1}/{self.n_splits}")
            
            # åˆ†å‰²æ•°æ®
            X_train_fold = X_train.iloc[train_idx]
            X_val_fold = X_train.iloc[val_idx]
            y_train_fold = y_train.iloc[train_idx]
            y_val_fold = y_train.iloc[val_idx]
            
            # å†…å±‚æ—¶åºäº¤å‰éªŒè¯è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜
            inner_cv = TimeSeriesSplit(n_splits=5)
            
            # ç²—æœç´¢
            model_coarse = xgb.XGBClassifier(random_state=42, n_jobs=-1, eval_metric='logloss', verbosity=0)
            search_coarse = RandomizedSearchCV(
                estimator=model_coarse,
                param_distributions=self.coarse_param_grid,
                n_iter=20,
                scoring='f1',
                cv=inner_cv,
                random_state=42,
                n_jobs=4,
                verbose=0
            )
            search_coarse.fit(X_train_fold, y_train_fold)
            
            # ç»†æœç´¢ (åŸºäºŽç²—æœç´¢ç»“æžœ)
            best_coarse = search_coarse.best_params_
            fine_grid = self.generate_fine_grid(best_coarse)
            
            model_fine = xgb.XGBClassifier(random_state=42, n_jobs=-1, eval_metric='logloss', verbosity=0)
            search_fine = RandomizedSearchCV(
                estimator=model_fine,
                param_distributions=fine_grid,
                n_iter=20,
                scoring='f1',
                cv=inner_cv,
                random_state=42,
                n_jobs=4,
                verbose=0
            )
            search_fine.fit(X_train_fold, y_train_fold)
            
            # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
            best_model = search_fine.best_estimator_
            y_pred_val = best_model.predict(X_val_fold)
            fold_score = f1_score(y_val_fold, y_pred_val)
            
            outer_scores.append(fold_score)
            best_params_list.append(search_fine.best_params_)
            
            if self.verbose:
                print(f"         F1: {fold_score:.4f}, å‚æ•°: {search_fine.best_params_}")
        
        # è¿”å›žå¹³å‡æœ€ä½³å‚æ•°
        avg_score = np.mean(outer_scores)
        std_score = np.std(outer_scores)
        
        self.log(f"   âœ… åµŒå¥—CVç»“æžœ: F1 = {avg_score:.4f} (Â±{std_score:.4f})")
        
        # é€‰æ‹©æœ€ä½³å‚æ•° (ç®€åŒ–ï¼šé€‰æ‹©ç¬¬ä¸€ä¸ªï¼Œå®žé™…å¯ä»¥ç”¨æŠ•ç¥¨)
        best_params = best_params_list[0] if best_params_list else self.coarse_param_grid
        
        return best_params, avg_score

    def generate_fine_grid(self, coarse_best):
        """åŸºäºŽç²—æœç´¢ç»“æžœç”Ÿæˆç²¾ç»†æœç´¢ç½‘æ ¼"""
        fine_grid = {}
        
        # n_estimators ç»†æœç´¢
        base_n_est = coarse_best.get('n_estimators', 200)
        fine_grid['n_estimators'] = [max(50, base_n_est - 50), base_n_est, base_n_est + 50]
        
        # max_depth ç»†æœç´¢
        base_depth = coarse_best.get('max_depth', 6)
        fine_grid['max_depth'] = [max(3, base_depth - 1), base_depth, base_depth + 1]
        
        # learning_rate ç»†æœç´¢
        base_lr = coarse_best.get('learning_rate', 0.1)
        fine_grid['learning_rate'] = [max(0.01, base_lr - 0.05), base_lr, base_lr + 0.05]
        
        # ä¿æŒæœ€ä½³çš„å…¶ä»–å‚æ•°
        fine_grid['subsample'] = [coarse_best.get('subsample', 0.8)]
        fine_grid['colsample_bytree'] = [coarse_best.get('colsample_bytree', 0.8)]
        fine_grid['reg_alpha'] = [coarse_best.get('reg_alpha', 0)]
        fine_grid['reg_lambda'] = [coarse_best.get('reg_lambda', 1)]
        fine_grid['min_child_weight'] = [coarse_best.get('min_child_weight', 1)]
        
        return fine_grid

    def train_model_v4(self, stock, task_type='direction', horizon='1D'):
        """è®­ç»ƒV4æ¨¡åž‹ - ç»ˆæžä¼˜åŒ–ç‰ˆ"""
        self.log(f"\nðŸš€ 2. è®­ç»ƒ {stock} æ¨¡åž‹ ({task_type}-{horizon})...")
        
        # å‡†å¤‡æ•°æ®
        X, y = self.prepare_data_no_leakage(stock, task_type, horizon)
        if X is None:
            return None
        
        # æœ€ç»ˆæµ‹è¯•é›†åˆ†å‰²
        split_idx = int(len(X) * (1 - self.test_size))
        X_train_full, X_test_full = X.iloc[:split_idx], X.iloc[split_idx:]
        y_train_full, y_test = y.iloc[:split_idx], y.iloc[split_idx:]
        
        self.log(f"   ðŸ“Š è®­ç»ƒé›†: {X_train_full.shape[0]} æ ·æœ¬")
        self.log(f"   ðŸ“Š æµ‹è¯•é›†: {X_test_full.shape[0]} æ ·æœ¬")
        
        # æ ‡å‡†åŒ– (åªåœ¨è®­ç»ƒé›†ä¸Šfit)
        scaler = StandardScaler()
        X_train_scaled = pd.DataFrame(
            scaler.fit_transform(X_train_full),
            columns=X_train_full.columns,
            index=X_train_full.index
        )
        X_test_scaled = pd.DataFrame(
            scaler.transform(X_test_full),
            columns=X_test_full.columns,
            index=X_test_full.index
        )
        self.scalers[stock] = scaler
        
        # åˆ†å±‚ç‰¹å¾é€‰æ‹©
        selected_features = self.hierarchical_feature_selection(X_train_scaled, y_train_full, stock)
        self.feature_selectors[stock] = selected_features
        
        X_train = X_train_scaled[selected_features]
        X_test = X_test_scaled[selected_features]
        
        # è¶…å‚æ•°ä¼˜åŒ–
        if self.nested_cv:
            best_params, cv_score = self.nested_cross_validation(X_train, y_train_full)
        else:
            # æ ‡å‡†æ–¹æ³•
            tscv = TimeSeriesSplit(n_splits=self.n_splits)
            model = xgb.XGBClassifier(random_state=42, n_jobs=4, eval_metric='logloss', verbosity=0, tree_method='hist', use_label_encoder=False)
            search = RandomizedSearchCV(
                estimator=model,
                param_distributions=self.coarse_param_grid,
                n_iter=30,
                scoring='f1',
                cv=tscv,
                random_state=42,
                n_jobs=1,
                verbose=0
            )
            search.fit(X_train, y_train_full)
            best_params = search.best_params_
            cv_score = search.best_score_
        
        self.log(f"   âœ… æœ€ä½³å‚æ•°: {best_params}")
        self.log(f"   âœ… CV F1å¾—åˆ†: {cv_score:.4f}")
        
        # è®­ç»ƒæœ€ç»ˆæ¨¡åž‹
        final_model = xgb.XGBClassifier(**best_params, random_state=42, n_jobs=4, eval_metric='logloss', verbosity=0, tree_method='hist', use_label_encoder=False)
        final_model.fit(X_train, y_train_full)
        
        # é¢„æµ‹å’Œè¯„ä¼°
        y_pred_train = final_model.predict(X_train)
        y_pred_test = final_model.predict(X_test)
        y_prob_train = final_model.predict_proba(X_train)[:, 1]
        y_prob_test = final_model.predict_proba(X_test)[:, 1]
        
        # è®¡ç®—æŒ‡æ ‡
        train_metrics = self.calculate_metrics(y_train_full, y_pred_train, y_prob_train)
        test_metrics = self.calculate_metrics(y_test, y_pred_test, y_prob_test)
        
        self.log(
            f"   âœ… è®­ç»ƒæŒ‡æ ‡: Acc={train_metrics['accuracy']:.4f}, F1={train_metrics['f1']:.4f}, AUC={train_metrics['roc_auc']:.4f}")
        self.log(
            f"   âœ… æµ‹è¯•æŒ‡æ ‡: Acc={test_metrics['accuracy']:.4f}, F1={test_metrics['f1']:.4f}, AUC={test_metrics['roc_auc']:.4f}")
        self.log(f"   âœ… PR-AUC: {test_metrics['pr_auc']:.4f}")
        
        # ç‰¹å¾é‡è¦æ€§
        feature_importance = pd.DataFrame({
            'feature': selected_features,
            'importance': final_model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        # ä¿å­˜ç»“æžœ
        self.models[stock] = {
            'model': final_model,
            'selected_features': selected_features,
            'train_metrics': train_metrics,
            'test_metrics': test_metrics,
            'feature_importance': feature_importance,
            'best_params': best_params,
            'cv_score': cv_score,
            'scaler': scaler
        }
        
        # ä¿å­˜æ¨¡åž‹
        dump({
            'model': final_model,
            'scaler': scaler,
            'selected_features': selected_features,
            'best_params': best_params
        }, f'xgb_v4_complete_{stock}.skops')
        
        return test_metrics

    def calculate_metrics(self, y_true, y_pred, y_prob):
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        return {
            'accuracy': accuracy_score(y_true, y_pred),
            'f1': f1_score(y_true, y_pred, average='binary'),
            'precision': precision_score(y_true, y_pred, average='binary', zero_division=0),
            'recall': recall_score(y_true, y_pred, average='binary', zero_division=0),
            'roc_auc': roc_auc_score(y_true, y_prob),
            'pr_auc': average_precision_score(y_true, y_prob)
        }

    def train_all_stocks_v4(self):
        """è®­ç»ƒæ‰€æœ‰è‚¡ç¥¨çš„V4æ¨¡åž‹"""
        self.log(f"\nðŸš€ å¼€å§‹è®­ç»ƒæ‰€æœ‰è‚¡ç¥¨çš„XGBoost V4æ¨¡åž‹...")
        
        summary = []
        
        # ä½¿ç”¨tqdmæ˜¾ç¤ºæ€»ä½“è¿›åº¦
        progress_bar = tqdm(self.banking_stocks, desc="è®­ç»ƒè¿›åº¦", disable=not self.verbose)
        
        for stock in progress_bar:
            progress_bar.set_description(f"è®­ç»ƒ {stock}")
            
            try:
                metrics = self.train_model_v4(stock)
                if metrics:
                    result_row = {'Stock': stock}
                    result_row.update(metrics)
                    summary.append(result_row)
                    
                    # æ›´æ–°è¿›åº¦æ¡åŽç¼€ä¿¡æ¯
                    progress_bar.set_postfix({
                        'Acc': f"{metrics['accuracy']:.3f}",
                        'F1': f"{metrics['f1']:.3f}",
                        'AUC': f"{metrics['roc_auc']:.3f}"
                    })
                    
            except Exception as e:
                self.log(f"   âŒ {stock} è®­ç»ƒå¤±è´¥: {e}", "ERROR")
                traceback.print_exc()
                continue
        
        # ä¿å­˜å’Œåˆ†æžç»“æžœ
        if summary:
            df_summary = pd.DataFrame(summary)
            df_summary.to_csv('xgb_v4_performance.csv', index=False)
            
            self.log(f"\nðŸ“Š XGBoost V4 æœ€ç»ˆç»“æžœ:")
            self.log(df_summary.round(4).to_string(index=False))
            
            # ç»Ÿè®¡æ‘˜è¦
            self.log(f"\nðŸ“ˆ å¹³å‡æ€§èƒ½ (Â±æ ‡å‡†å·®):")
            for metric in ['accuracy', 'f1', 'roc_auc', 'pr_auc']:
                mean_val = df_summary[metric].mean()
                std_val = df_summary[metric].std()
                self.log(f"   {metric.upper()}: {mean_val:.4f} (Â±{std_val:.4f})")
            
            # æœ€ä½³è¡¨çŽ°è‚¡ç¥¨
            best_stock = df_summary.loc[df_summary['f1'].idxmax()]
            self.log(f"\nðŸ† æœ€ä½³è¡¨çŽ°: {best_stock['Stock']} (F1: {best_stock['f1']:.4f})")
        
        return summary


def main():
    """ä¸»å‡½æ•°"""
    print("ðŸš€ å¯åŠ¨ XGBoost V4 ç»ˆæžä¼˜åŒ–ç‰ˆ...")
    
    # åˆ›å»ºæ¨¡åž‹
    xgb_v4 = BankingXGBoostV4(
        top_k_features=50,
        n_splits=5,
        test_size=0.2,
        pre_rfe_features=200,  # RFEå‰é¢„ç­›é€‰ç‰¹å¾æ•°
        nested_cv=False,  # å¯ç”¨åµŒå¥—äº¤å‰éªŒè¯
        verbose=True  # æ˜¾ç¤ºè¯¦ç»†è¿›åº¦
    )
    
    # åŠ è½½æ•°æ®
    if not xgb_v4.load_data():
        return None
    
    # è®­ç»ƒæ‰€æœ‰è‚¡ç¥¨
    performance_summary = xgb_v4.train_all_stocks_v4()
    
    print("\n" + "=" * 80)
    print("ðŸŽ‰ XGBoost V4 ç»ˆæžä¼˜åŒ–ç‰ˆè®­ç»ƒå®Œæˆ!")
    print("=" * 80)
    print("ðŸ“Š ç”Ÿæˆæ–‡ä»¶:")
    print("   ðŸ“„ xgb_v4_performance.csv - æœ€ç»ˆæ€§èƒ½æŠ¥å‘Š")
    print("   ðŸ“„ xgb_v4_complete_*.skops - å®Œæ•´æ¨¡åž‹åŒ… (æ¨¡åž‹+é¢„å¤„ç†+ç‰¹å¾)")
    
    return xgb_v4


if __name__ == "__main__":
    model_v4 = main()