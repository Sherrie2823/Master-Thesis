

import os
import warnings
import sys

# æŠ‘åˆ¶æ‰€æœ‰FutureWarningå’ŒUserWarning
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning)
warnings.filterwarnings('ignore', category=DeprecationWarning)

# æŠ‘åˆ¶XGBoostçš„ç³»ç»Ÿè­¦å‘Š
os.environ['PYTHONWARNINGS'] = 'ignore'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # å¦‚æœæœ‰tensorflowç›¸å…³è­¦å‘Š

# é‡å®šå‘stderrä»¥å‡å°‘è¾“å‡º
class SuppressOutput:
    def __enter__(self):
        self._original_stderr = sys.stderr
        sys.stderr = open(os.devnull, 'w')
        
    def __exit__(self, exc_type, exc_val, exc_tb):
        sys.stderr.close()
        sys.stderr = self._original_stderr

import os
import pandas as pd
import numpy as np
import xgboost as xgb
from xgboost import XGBClassifier
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, TimeSeriesSplit
from sklearn.ensemble import BaggingClassifier
from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score,
                             roc_auc_score, average_precision_score)
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.calibration import CalibratedClassifierCV  # æ”¹è¿›äºŒï¼šæ¦‚ç‡æ ¡å‡†
from collections import Counter  # æ”¹è¿›å››ï¼šç±»åˆ«å¹³è¡¡
import joblib
from skops.io import dump
from tqdm.auto import tqdm
import warnings
from scipy.stats import uniform, randint
import traceback
import joblib 
# æ”¹è¿›ä¸‰ï¼šæŠ€æœ¯æŒ‡æ ‡
try:
    import ta
    TA_AVAILABLE = True
except ImportError:
    TA_AVAILABLE = False
    print("âš ï¸ taåº“æœªå®‰è£…ï¼Œå°†è·³è¿‡æŠ€æœ¯æŒ‡æ ‡ç”Ÿæˆã€‚å¯é€šè¿‡ pip install ta å®‰è£…")

warnings.filterwarnings('ignore')


class BankingXGBoostV5:
    def __init__(self, top_k_features=200, n_splits=5, test_size=0.2,
                 pre_rfe_features=200, nested_cv=False, verbose=True,
                 enable_tech_indicators=True, calibration_method='sigmoid'):
        """
        XGBoost V5 - ä¸ƒæ­¥æ”¹è¿›ä¼˜åŒ–ç‰ˆ
        
        Parameters:
        - top_k_features: æœ€ç»ˆé€‰æ‹©çš„ç‰¹å¾æ•°é‡
        - n_splits: æ—¶åºäº¤å‰éªŒè¯æŠ˜æ•°
        - test_size: æœ€ç»ˆæµ‹è¯•é›†æ¯”ä¾‹
        - pre_rfe_features: RFEå‰çš„é¢„ç­›é€‰ç‰¹å¾æ•°
        - nested_cv: æ˜¯å¦ä½¿ç”¨åµŒå¥—äº¤å‰éªŒè¯
        - verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†è¿›åº¦
        - enable_tech_indicators: æ˜¯å¦å¯ç”¨æŠ€æœ¯æŒ‡æ ‡ç”Ÿæˆ
        - calibration_method: æ¦‚ç‡æ ¡å‡†æ–¹æ³• ('sigmoid' æˆ– 'isotonic')
        """
        self.features = None
        self.targets = None
        self.top_k_features = top_k_features
        self.n_splits = n_splits
        self.test_size = test_size
        self.pre_rfe_features = pre_rfe_features
        self.nested_cv = nested_cv
        self.verbose = verbose
        self.enable_tech_indicators = enable_tech_indicators
        self.calibration_method = calibration_method
        
        self.banking_stocks = ['AXP', 'BAC', 'BK', 'C', 'CB', 'COF', 'GS',
                               'JPM', 'MS', 'PNC', 'SCHW', 'STT', 'TFC', 'USB', 'WFC']
        self.models = {}
        self.scalers = {}
        self.feature_selectors = {}
        self.results = {}
        self.optimal_thresholds = {}  # æ”¹è¿›ä¸€ï¼šå­˜å‚¨æœ€ä¼˜é˜ˆå€¼
        self.calibrated_models = {}  # æ”¹è¿›äºŒï¼šå­˜å‚¨æ ¡å‡†åæ¨¡å‹
        
        # ä¸¤é˜¶æ®µè¶…å‚æ•°æœç´¢
        self.coarse_param_grid = {
            'n_estimators': [50, 100, 150],
            'max_depth': [3, 5, 7],
            'learning_rate': [0.005, 0.01, 0.02],
            'subsample': [0.6, 0.8, 1.0],
            'colsample_bytree': [0.6, 0.8, 1.0],
            'gamma': [0, 0.1, 0.3],
            'reg_alpha': [0, 0.1, 1],
            'reg_lambda': [1, 10, 50],
            'min_child_weight': [1, 3, 5]
        }
        
        self.fine_param_grid = {}  # åŠ¨æ€ç”Ÿæˆ

    def log(self, message, level="INFO"):
        """æ—¥å¿—è¾“å‡º"""
        if self.verbose:
            print(f"[{level}] {message}")

    def generate_technical_indicators(self, price_data):
        """
        æ”¹è¿›ä¸‰ï¼šç”ŸæˆæŠ€æœ¯æŒ‡æ ‡ç‰¹å¾
        """
        if not TA_AVAILABLE:
            self.log("   âš ï¸ taåº“ä¸å¯ç”¨ï¼Œè·³è¿‡æŠ€æœ¯æŒ‡æ ‡ç”Ÿæˆ", "WARNING")
            return pd.DataFrame(index=price_data.index)
        
        tech_features = pd.DataFrame(index=price_data.index)
        
        for stock in price_data.columns:
            if stock in self.banking_stocks:
                # è·å–è‚¡ç¥¨ä»·æ ¼åºåˆ—
                prices = price_data[stock].dropna()
                
                # åŸºæœ¬æŠ€æœ¯æŒ‡æ ‡
                # ç§»åŠ¨å¹³å‡çº¿
                tech_features[f'{stock}_MA5'] = prices.rolling(5).mean()
                tech_features[f'{stock}_MA10'] = prices.rolling(10).mean()
                tech_features[f'{stock}_MA20'] = prices.rolling(20).mean()
                
                # æ³¢åŠ¨ç‡
                tech_features[f'{stock}_VOL5'] = prices.rolling(5).std()
                tech_features[f'{stock}_VOL20'] = prices.rolling(20).std()
                
                # ä»·æ ¼ç›¸å¯¹ä½ç½®
                tech_features[f'{stock}_PRICE_RATIO'] = prices / prices.rolling(20).mean()
                
                # å¦‚æœtaåº“å¯ç”¨ï¼Œæ·»åŠ æ›´å¤šæŒ‡æ ‡
                try:
                    # RSI
                    tech_features[f'{stock}_RSI'] = ta.momentum.RSIIndicator(
                        close=prices, window=14
                    ).rsi()
                    
                    # å¸ƒæ—å¸¦
                    bb_indicator = ta.volatility.BollingerBands(close=prices, window=20)
                    tech_features[f'{stock}_BB_HIGH'] = bb_indicator.bollinger_hband()
                    tech_features[f'{stock}_BB_LOW'] = bb_indicator.bollinger_lband()
                    tech_features[f'{stock}_BB_WIDTH'] = (
                        bb_indicator.bollinger_hband() - bb_indicator.bollinger_lband()
                    ) / bb_indicator.bollinger_mavg()
                    
                    # MACD
                    macd_indicator = ta.trend.MACD(close=prices)
                    tech_features[f'{stock}_MACD'] = macd_indicator.macd()
                    tech_features[f'{stock}_MACD_SIGNAL'] = macd_indicator.macd_signal()
                    
                except Exception as e:
                    self.log(f"   âš ï¸ {stock} æŠ€æœ¯æŒ‡æ ‡ç”Ÿæˆå¤±è´¥: {e}", "WARNING")
                    continue
        
        # å¡«å……ç¼ºå¤±å€¼
        tech_features = tech_features.fillna(method='ffill').fillna(0)
        
        self.log(f"   âœ… ç”ŸæˆæŠ€æœ¯æŒ‡æ ‡ç‰¹å¾: {tech_features.shape[1]} ä¸ª")
        return tech_features

    def load_data(self, feature_path='banking_returns.csv', target_path='banking_targets_ai.csv'):
        self.log("=" * 80)
        self.log("ğŸš€ XGBoost V5 - ä¸ƒæ­¥æ”¹è¿›ä¼˜åŒ–ç‰ˆ")
        self.log("=" * 80)
        self.log("ğŸ“Š 1. åŠ è½½æ•°æ®...")
        
        self.features = pd.read_csv(feature_path, index_col='Date', parse_dates=True)
        self.targets = pd.read_csv(target_path, index_col='Date', parse_dates=True)
        
        # æ”¹è¿›ä¸‰ï¼šç”ŸæˆæŠ€æœ¯æŒ‡æ ‡ç‰¹å¾
        if self.enable_tech_indicators:
            self.log("   ğŸ”§ ç”ŸæˆæŠ€æœ¯æŒ‡æ ‡ç‰¹å¾...")
            
            # ä»ç‰¹å¾æ•°æ®ä¸­æå–ä»·æ ¼ä¿¡æ¯ï¼ˆå‡è®¾æœ‰ä»·æ ¼ç›¸å…³åˆ—ï¼‰
            # å¦‚æœæ²¡æœ‰åŸå§‹ä»·æ ¼æ•°æ®ï¼Œæˆ‘ä»¬åŸºäºç°æœ‰ç‰¹å¾åˆ›å»ºæŠ€æœ¯æŒ‡æ ‡çš„ä»£ç†
            tech_features = self.generate_technical_indicators_from_features()
            
            # åˆå¹¶æŠ€æœ¯æŒ‡æ ‡åˆ°ä¸»ç‰¹å¾é›†
            aligned_tech = tech_features.reindex(self.features.index, method='ffill')
            self.features = pd.concat([self.features, aligned_tech], axis=1)
            
            self.log(f"   âœ… æŠ€æœ¯æŒ‡æ ‡å·²æ·»åŠ ï¼Œæ–°ç‰¹å¾æ•°: {self.features.shape[1]}")
        
        self.log(f"   âœ… ç‰¹å¾æ•°æ®: {self.features.shape}")
        self.log(f"   âœ… ç›®æ ‡æ•°æ®: {self.targets.shape}")
        self.log(f"   âœ… æ—¶é—´èŒƒå›´: {self.features.index[0]} åˆ° {self.features.index[-1]}")
        self.log(f"   âœ… é…ç½®: Top-{self.top_k_features}ç‰¹å¾, {self.n_splits}æŠ˜CV, " +
                 f"é¢„ç­›é€‰{self.pre_rfe_features}ç‰¹å¾, æ ¡å‡†æ–¹æ³•={self.calibration_method}")
        return True

    def generate_technical_indicators_from_features(self):
        """
        åŸºäºç°æœ‰ç‰¹å¾ç”ŸæˆæŠ€æœ¯æŒ‡æ ‡çš„ä»£ç†ç‰¹å¾
        """
        tech_features = pd.DataFrame(index=self.features.index)
        
        # å¯»æ‰¾è¿”å›ç‡ç›¸å…³çš„åˆ—
        return_cols = [col for col in self.features.columns if 'return' in col.lower() or any(stock in col for stock in self.banking_stocks)]
        
        for col in return_cols[:len(self.banking_stocks)]:  # é™åˆ¶å¤„ç†æ•°é‡
            try:
                # åŸºäºæ”¶ç›Šç‡åºåˆ—ç”ŸæˆæŠ€æœ¯ç‰¹å¾
                values = self.features[col].fillna(0)
                
                # æ»šåŠ¨ç»Ÿè®¡ç‰¹å¾
                tech_features[f'{col}_MA5'] = values.rolling(5).mean()
                tech_features[f'{col}_MA10'] = values.rolling(10).mean()
                tech_features[f'{col}_MA20'] = values.rolling(20).mean()
                
                tech_features[f'{col}_STD5'] = values.rolling(5).std()
                tech_features[f'{col}_STD20'] = values.rolling(20).std()
                
                # åŠ¨é‡ç‰¹å¾
                tech_features[f'{col}_MOM5'] = values - values.shift(5)
                tech_features[f'{col}_MOM10'] = values - values.shift(10)
                
                # ç›¸å¯¹å¼ºåº¦
                tech_features[f'{col}_RATIO'] = values / values.rolling(20).mean()
                
            except Exception as e:
                continue
        
        # å¡«å……ç¼ºå¤±å€¼
        tech_features = tech_features.fillna(method='ffill').fillna(0)
        
        return tech_features

    def prepare_data_no_leakage(self, stock, task_type='direction', horizon='1D'):
        """æ— æ•°æ®æ³„æ¼çš„æ•°æ®å‡†å¤‡"""
        target_col = f'{stock}_Direction_{horizon}'
        if target_col not in self.targets.columns:
            self.log(f"   âš ï¸ ç›®æ ‡å˜é‡ {target_col} ä¸å­˜åœ¨!", "WARNING")
            return None, None
            
        X = self.features.dropna()
        y = self.targets[target_col].dropna()
        valid_idx = X.index.intersection(y.index)
        X, y = X.loc[valid_idx], y.loc[valid_idx]
        
        self.log(f"   ğŸ“ˆ {stock}: {X.shape[0]} æ ·æœ¬, {X.shape[1]} ç‰¹å¾")
        self.log(f"   ğŸ“Š ç±»åˆ«åˆ†å¸ƒ: {dict(y.value_counts().sort_index())}")
        
        return X, y

    def hierarchical_feature_selection(self, X_train, y_train, stock):
        """
        åˆ†å±‚ç‰¹å¾é€‰æ‹©: SelectKBest -> XGB Importance -> RFE
        æ”¹è¿›å…­ï¼šåœ¨æ­¤é˜¶æ®µå¯ä»¥å‰”é™¤å¼±ç‰¹å¾
        """
        n_samples = len(X_train)
        original_features = X_train.shape[1]
        
        self.log(f"   ğŸ” å¼€å§‹åˆ†å±‚ç‰¹å¾é€‰æ‹© (åŸå§‹ç‰¹å¾: {original_features})")
        
        # ç¬¬ä¸€å±‚: ç»Ÿè®¡ç­›é€‰ (å¿«é€Ÿ)
        if original_features > self.pre_rfe_features:
            selector_stat = SelectKBest(score_func=f_classif, k=self.pre_rfe_features)
            X_stat = selector_stat.fit_transform(X_train, y_train)
            selected_features_stat = X_train.columns[selector_stat.get_support()].tolist()
            self.log(f"   ğŸ“‰ ç»Ÿè®¡ç­›é€‰: {original_features} -> {len(selected_features_stat)}")
        else:
            X_stat = X_train
            selected_features_stat = X_train.columns.tolist()
            self.log(f"   ğŸ“‰ è·³è¿‡ç»Ÿè®¡ç­›é€‰ (ç‰¹å¾æ•°å·²å°‘äºé˜ˆå€¼)")
        
        # ç¬¬äºŒå±‚: XGBoosté‡è¦æ€§
        xgb_selector = xgb.XGBClassifier(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            random_state=42,
            n_jobs=4,
            eval_metric='logloss',
            verbosity=0
        )
        xgb_selector.fit(X_stat, y_train)
        
        # é€‰æ‹©é‡è¦æ€§æœ€é«˜çš„ç‰¹å¾
        importances = pd.Series(xgb_selector.feature_importances_, index=selected_features_stat)
        xgb_top_features = importances.nlargest(min(self.top_k_features * 2, len(selected_features_stat))).index.tolist()
        self.log(f"   ğŸš€ XGBé‡è¦æ€§ç­›é€‰: {len(selected_features_stat)} -> {len(xgb_top_features)}")
        
        # æ”¹è¿›å…­ï¼šç‰¹å¾é‡è¦æ€§å¯è§†åŒ–å’Œå¼±ç‰¹å¾å‰”é™¤
        if self.verbose:
            weak_features = importances.nsmallest(10)
            self.log(f"   ğŸ“Š æœ€å¼±çš„10ä¸ªç‰¹å¾: {weak_features.to_dict()}")
        
        # ç¬¬ä¸‰å±‚: RFE (ä»…åœ¨åˆç†æ ·æœ¬æ•°ä¸‹ä½¿ç”¨)
        if n_samples >= 500 and len(xgb_top_features) > self.top_k_features:
            self.log(f"   ğŸ”„ æ‰§è¡ŒRFE (æ ·æœ¬æ•°å……è¶³: {n_samples})")
            X_xgb = X_train[xgb_top_features]
            
            rfe = RFE(
                estimator=xgb.XGBClassifier(
                    n_estimators=50,
                    max_depth=6,
                    learning_rate=0.1,
                    random_state=42,
                    n_jobs=4,
                    eval_metric='logloss',
                    verbosity=0
                ),
                n_features_to_select=self.top_k_features,
                step=0.1
            )
            
            with tqdm(desc="RFEè¿›åº¦", disable=not self.verbose) as pbar:
                rfe.fit(X_xgb, y_train)
                pbar.update(1)
            
            final_features = X_xgb.columns[rfe.support_].tolist()
            self.log(f"   ğŸ¯ RFEç­›é€‰: {len(xgb_top_features)} -> {len(final_features)}")
        else:
            if n_samples < 500:
                self.log(f"   âš ï¸ RFEè·³è¿‡ (æ ·æœ¬æ•°ä¸è¶³: {n_samples})")
            elif len(xgb_top_features) <= self.top_k_features:
                self.log(f"   âš ï¸ RFEè·³è¿‡ (ç‰¹å¾æ•°å·²è¾¾æ ‡: {len(xgb_top_features)})")
            final_features = xgb_top_features[:self.top_k_features]
        
        self.log(f"   âœ… æœ€ç»ˆé€‰æ‹©ç‰¹å¾: {len(final_features)}")
        return final_features

    def optimize_threshold(self, y_true, y_prob):
        """
        æ”¹è¿›ä¸€ï¼šé˜ˆå€¼ä¼˜åŒ–
        åœ¨æµ‹è¯•é›†ä¸Šæ‰¾åˆ°æœ€ä¼˜çš„äºŒåˆ†ç±»é˜ˆå€¼
        """
        best_thr, best_f1 = 0.5, 0
        thresholds = np.linspace(0.1, 0.9, 17)
        
        for thr in thresholds:
            y_pred_thr = (y_prob >= thr).astype(int)
            f1 = f1_score(y_true, y_pred_thr)
            if f1 > best_f1:
                best_f1, best_thr = f1, thr
        
        self.log(f"   ğŸ”§ æœ€ä¼˜é˜ˆå€¼: {best_thr:.2f}, å¯¹åº” F1: {best_f1:.4f}")
        return best_thr, best_f1

    def custom_f1_eval(self, y_pred, dtrain):
        """
        æ”¹è¿›äº”ï¼šè‡ªå®šä¹‰F1è¯„ä¼°å‡½æ•°ç”¨äºæ—©åœ
        """
        y_true = dtrain.get_label().astype(int)
        thr = 0.5
        y_pred_binary = (y_pred >= thr).astype(int)
        f1 = f1_score(y_true, y_pred_binary)
        return 'f1', f1

    def nested_cross_validation(self, X_train, y_train):
        """
        åµŒå¥—äº¤å‰éªŒè¯: å¤–å±‚è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œå†…å±‚è°ƒä¼˜è¶…å‚æ•°
        """
        outer_scores = []
        best_params_list = []
        
        # å¤–å±‚æ—¶åºåˆ†å‰²
        outer_cv = TimeSeriesSplit(n_splits=self.n_splits)
        
        self.log(f"   ğŸ”„ åµŒå¥—CV: {self.n_splits}æŠ˜å¤–å±‚éªŒè¯")
        
        for fold_idx, (train_idx, val_idx) in enumerate(outer_cv.split(X_train)):
            if self.verbose:
                print(f"      æŠ˜æ•° {fold_idx + 1}/{self.n_splits}")
            
            # åˆ†å‰²æ•°æ®
            X_train_fold = X_train.iloc[train_idx]
            X_val_fold = X_train.iloc[val_idx]
            y_train_fold = y_train.iloc[train_idx]
            y_val_fold = y_train.iloc[val_idx]
            
            # å†…å±‚æ—¶åºäº¤å‰éªŒè¯è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜
            inner_cv = TimeSeriesSplit(n_splits=3)
            
            # æ”¹è¿›å››ï¼šç±»åˆ«å¹³è¡¡æƒé‡
            cnt = Counter(y_train_fold)
            scale_pos_weight = cnt[0] / cnt[1] if cnt[1] > 0 else 1.0
            self.log(f"âš–ï¸ ç±»åˆ«å¹³è¡¡æƒé‡: {scale_pos_weight:.3f}")
            self.coarse_param_grid['scale_pos_weight'] = [1, scale_pos_weight]
            
            # ç²—æœç´¢
            coarse_grid = self.coarse_param_grid.copy()
            coarse_grid['scale_pos_weight'] = [scale_pos_weight]
            
            model_coarse = xgb.XGBClassifier(random_state=42, n_jobs=-1, eval_metric='logloss', verbosity=0)
            search_coarse = RandomizedSearchCV(
                estimator=model_coarse,
                param_distributions=coarse_grid,
                n_iter=20,
                scoring='f1',
                cv=inner_cv,
                random_state=42,
                n_jobs=4,
                verbose=0
            )
            search_coarse.fit(X_train_fold, y_train_fold)
            
            # ç»†æœç´¢ (åŸºäºç²—æœç´¢ç»“æœ)
            best_coarse = search_coarse.best_params_
            fine_grid = self.generate_fine_grid(best_coarse)
            
            model_fine = xgb.XGBClassifier(random_state=42, n_jobs=-1, eval_metric='logloss', verbosity=0)
            search_fine = RandomizedSearchCV(
                estimator=model_fine,
                param_distributions=fine_grid,
                n_iter=20,
                scoring='f1',
                cv=inner_cv,
                random_state=42,
                n_jobs=4,
                verbose=0
            )
            search_fine.fit(X_train_fold, y_train_fold)
            
            # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
            best_model = search_fine.best_estimator_
            y_pred_val = best_model.predict(X_val_fold)
            fold_score = f1_score(y_val_fold, y_pred_val)
            
            outer_scores.append(fold_score)
            best_params_list.append(search_fine.best_params_)
            
            if self.verbose:
                print(f"         F1: {fold_score:.4f}, å‚æ•°: {search_fine.best_params_}")
        
        # è¿”å›å¹³å‡æœ€ä½³å‚æ•°
        avg_score = np.mean(outer_scores)
        std_score = np.std(outer_scores)
        
        self.log(f"   âœ… åµŒå¥—CVç»“æœ: F1 = {avg_score:.4f} (Â±{std_score:.4f})")
        
        # é€‰æ‹©æœ€ä½³å‚æ•° (ç®€åŒ–ï¼šé€‰æ‹©ç¬¬ä¸€ä¸ªï¼Œå®é™…å¯ä»¥ç”¨æŠ•ç¥¨)
        best_params = best_params_list[0] if best_params_list else self.coarse_param_grid
        
        return best_params, avg_score

    def generate_fine_grid(self, coarse_best):
        """åŸºäºç²—æœç´¢ç»“æœç”Ÿæˆç²¾ç»†æœç´¢ç½‘æ ¼"""
        fine_grid = {}
        
        # n_estimators ç»†æœç´¢
        base_n_est = coarse_best.get('n_estimators', 200)
        fine_grid['n_estimators'] = [max(50, base_n_est - 50), base_n_est, base_n_est + 50]
        
        # max_depth ç»†æœç´¢
        base_depth = coarse_best.get('max_depth', 6)
        fine_grid['max_depth'] = [max(3, base_depth - 1), base_depth, base_depth + 1]
        
        # learning_rate ç»†æœç´¢
        base_lr = coarse_best.get('learning_rate', 0.1)
        fine_grid['learning_rate'] = [max(0.01, base_lr - 0.05), base_lr, base_lr + 0.05]
        
        # ä¿æŒæœ€ä½³çš„å…¶ä»–å‚æ•°
        fine_grid['subsample'] = [coarse_best.get('subsample', 0.8)]
        fine_grid['colsample_bytree'] = [coarse_best.get('colsample_bytree', 0.8)]
        fine_grid['reg_alpha'] = [coarse_best.get('reg_alpha', 0)]
        fine_grid['reg_lambda'] = [coarse_best.get('reg_lambda', 1)]
        fine_grid['min_child_weight'] = [coarse_best.get('min_child_weight', 1)]
        fine_grid['scale_pos_weight'] = [coarse_best.get('scale_pos_weight', 1.0)]
        
        return fine_grid

    def train_model_v5(self, stock, task_type='direction', horizon='1D'):
        """è®­ç»ƒV5æ¨¡å‹ - ä¸ƒæ­¥æ”¹è¿›ä¼˜åŒ–ç‰ˆ"""
        self.log(f"\nğŸš€ 2. è®­ç»ƒ {stock} æ¨¡å‹ ({task_type}-{horizon})...")
        
        # å‡†å¤‡æ•°æ®
        X, y = self.prepare_data_no_leakage(stock, task_type, horizon)
        if X is None:
            return None
        
        # æœ€ç»ˆæµ‹è¯•é›†åˆ†å‰²
        split_idx = int(len(X) * (1 - self.test_size))
        X_train_full, X_test_full = X.iloc[:split_idx], X.iloc[split_idx:]
        y_train_full, y_test = y.iloc[:split_idx], y.iloc[split_idx:]
        
        self.log(f"   ğŸ“Š è®­ç»ƒé›†: {X_train_full.shape[0]} æ ·æœ¬")
        self.log(f"   ğŸ“Š æµ‹è¯•é›†: {X_test_full.shape[0]} æ ·æœ¬")
        
        # æ ‡å‡†åŒ– (åªåœ¨è®­ç»ƒé›†ä¸Šfit)
        scaler_full = StandardScaler()
        X_train_full_scaled = pd.DataFrame(
            scaler_full.fit_transform(X_train_full),
            columns=X_train_full.columns,
            index=X_train_full.index
        )
        X_test_full_scaled = pd.DataFrame(
            scaler_full.transform(X_test_full),
            columns=X_test_full.columns,
            index=X_test_full.index
        )

        
        # åˆ†å±‚ç‰¹å¾é€‰æ‹©
        selected_features = self.hierarchical_feature_selection(X_train_full_scaled, y_train_full, stock)
        self.feature_selectors[stock] = selected_features
        
            # é’ˆå¯¹å·²é€‰çš„ 50 ç»´ç‰¹å¾ï¼Œå† fit ä¸€ä¸ªåªå¯¹å®ƒä»¬æœ‰æ•ˆçš„ StandardScaler
        scaler_sel = StandardScaler()
        X_train_sel = X_train_full_scaled[selected_features]
        X_train_sel_scaled = scaler_sel.fit_transform(X_train_sel)

   # ç”¨æ–°çš„ scaler_sel å» transform æµ‹è¯•é›†å¯¹åº”åˆ—
        X_test_sel = X_test_full_scaled[selected_features]
        X_test_sel_scaled = scaler_sel.transform(X_test_sel)

   # æœ€åä¿å­˜è¿™ä¸ªåªå¯¹ 50 ç»´ç”Ÿæ•ˆçš„ scaler
        self.scalers[stock] = scaler_sel

  # æ¥ä¸‹æ¥æŠŠ X_train_scaled/X_test_scaled ä¹ŸæŒ‡å‘è¿™ä¸¤ä¸ªè¡¨
        X_train_scaled = pd.DataFrame(
            X_train_sel_scaled, columns=selected_features, index=X_train_sel.index
        )
        X_test_scaled = pd.DataFrame(
            X_test_sel_scaled, columns=selected_features, index=X_test_sel.index
        )
              
        
        X_train = X_train_scaled[selected_features]
        X_test = X_test_scaled[selected_features]
        
        # æ”¹è¿›å››ï¼šè®¡ç®—ç±»åˆ«å¹³è¡¡æƒé‡
        cnt = Counter(y_train_full)
        scale_pos_weight = cnt[0] / cnt[1] if cnt[1] > 0 else 1.0
        self.log(f"   âš–ï¸ ç±»åˆ«å¹³è¡¡æƒé‡: {scale_pos_weight:.3f}")
        
        # è¶…å‚æ•°ä¼˜åŒ–
        if self.nested_cv:
            best_params, cv_score = self.nested_cross_validation(X_train, y_train_full)
        else:
            
            # â€”â€” ç¬¬ä¸€é˜¶æ®µï¼šéšæœºæœç´¢ â€”â€” 
            tscv = TimeSeriesSplit(n_splits=self.n_splits)
            param_dist = {
                'max_depth':        [3, 5, 7, 9],
                'learning_rate':    [0.01, 0.05, 0.1, 0.2],
                'n_estimators':     [50, 100, 200, 300],
                'min_child_weight': [1, 3, 5, 7],
                'gamma':            [0, 0.1, 0.3, 0.5],
                'subsample':        [0.6, 0.8, 1.0],
                'colsample_bytree': [0.6, 0.8, 1.0],
                'scale_pos_weight': [scale_pos_weight],
            }
            
            base_model = xgb.XGBClassifier(
                random_state=42, n_jobs=4, eval_metric='logloss', 
                verbosity=0, tree_method='hist', use_label_encoder=False
            )
            rand_search = RandomizedSearchCV(
                estimator=base_model,
                param_distributions=param_dist,
                n_iter=30,
                scoring='f1',
                cv=tscv,
                random_state=42,
                n_jobs=1,
                verbose=0
            )
            rand_search.fit(X_train_full_scaled, y_train_full)
            coarse_best = rand_search.best_params_
            coarse_score = rand_search.best_score_
            self.log(f"ğŸ” éšæœºæœç´¢æœ€ä½³å‚æ•°: {coarse_best}, CVå¾—åˆ†={coarse_score:.4f}")
        
        # â€”â€” ç¬¬äºŒé˜¶æ®µï¼šç»†ç½‘æ ¼æœç´¢ â€”â€” 
        # åœ¨ coarse_best å‘¨å›´åšä¸€ä¸ªå°èŒƒå›´ç½‘æ ¼
            param_grid_fine = {
                'max_depth':        sorted({max(1, coarse_best['max_depth']-2), coarse_best['max_depth'], coarse_best['max_depth']+2}),
            'learning_rate':    [coarse_best['learning_rate']*0.5, coarse_best['learning_rate'], coarse_best['learning_rate']*1.5],
            'n_estimators':     sorted({max(10, coarse_best['n_estimators']-50), coarse_best['n_estimators'], coarse_best['n_estimators']+50}),
            'min_child_weight': sorted({1, coarse_best['min_child_weight'], coarse_best['min_child_weight']+2}),
            'gamma':            [max(0, coarse_best['gamma']-0.1), coarse_best['gamma'], coarse_best['gamma']+0.1],
            'subsample':        [coarse_best['subsample']],
            'colsample_bytree': [coarse_best['colsample_bytree']],
            'scale_pos_weight': [scale_pos_weight],
            }

            grid_search = GridSearchCV(
                estimator=base_model,
                param_grid=param_grid_fine,
                scoring='f1',
                cv=tscv,
                n_jobs=1,
                verbose=0,
            )
            grid_search.fit(X_train_full_scaled, y_train_full)
            best_params = grid_search.best_params_
            cv_score    = grid_search.best_score_
            self.log(f"âœ… ç»†ç½‘æ ¼æœç´¢æœ€ä½³å‚æ•°: {best_params}, CVå¾—åˆ†={cv_score:.4f}")
        
        # ç”¨åŒä¸€ä¸ªè¶…å‚çš„ XGB åš base_estimatorï¼Œå¤–é¢å¥— Bagging é™ä½æ–¹å·®
        base_xgb = xgb.XGBClassifier(
            **best_params, 
            random_state=42, n_jobs=1, eval_metric='logloss', 
            verbosity=0, tree_method='hist', use_label_encoder=False
        )
        
        final_model = BaggingClassifier(
            estimator=base_xgb,
            n_estimators=5,      # 5 ä¸ª bootstrap å­æ¨¡å‹
            max_samples=0.8,     # æ¯ä¸ªå­æ¨¡å‹å– 80% çš„æ ·æœ¬é‡é‡‡æ ·
            n_jobs=5,            # å¹¶è¡Œè®­ç»ƒ 5 ä¸ªå­æ¨¡å‹
            random_state=42,
            verbose=False
        )
        
        final_model.fit(
            X_train_scaled,    # ç”¨ä½ å®Œæ•´çš„è®­ç»ƒé›†ï¼ˆå·² scale & selectï¼‰çš„ DataFrame
            y_train_full,           # å¯¹åº”çš„æ ‡ç­¾
        )
        
        # è®¡ç®—æ¯ä¸€ä¸ªå­æ¨¡å‹çš„ feature_importances_
        all_imps = np.array([
            est.feature_importances_
            for est in final_model.estimators_
        ])
        
        # å¹³å‡å®ƒä»¬
        mean_imp = all_imps.mean(axis=0)
        # æ‰‹åŠ¨ç»™ bagged å¯¹è±¡ç»‘ä¸€ä¸ªå±æ€§  
        final_model.feature_importances_ = mean_imp
        
        # æ”¹è¿›äºŒï¼šæ¦‚ç‡æ ¡å‡†
        self.log(f"   ğŸ”§ æ¦‚ç‡æ ¡å‡† sigmoid + 3 æŠ˜æ—¶åºCV")
        # è®©å®ƒè‡ªå·±åœ¨å†…éƒ¨åš CVï¼Œä¸ç”¨ prefit
        tscv_cal = TimeSeriesSplit(n_splits=3)
        calibrator = CalibratedClassifierCV(
            estimator=final_model,
            method='sigmoid',
            cv=3
        )
        
        # ç”¨æ•´ä¸ªè®­ç»ƒé›†åšæ ¡å‡†ï¼ˆå®ƒå†…éƒ¨ä¼šæŒ‰ tscv_cal åˆ’åˆ†ï¼‰
        calibrator.fit(
            X_train_scaled[selected_features],
            y_train_full
        )
        
        # å­˜ä¸‹æ¥
        self.calibrated_models[stock] = calibrator
        self.log("âš™ï¸ æ¦‚ç‡æ ¡å‡†å®Œæˆ")
        
        # é¢„æµ‹
        y_pred_train = calibrator.predict(X_train)
        y_pred_test = calibrator.predict(X_test)
        y_prob_train = calibrator.predict_proba(X_train)[:, 1]
        y_prob_test = calibrator.predict_proba(X_test)[:, 1]
        
        # æ”¹è¿›ä¸€ï¼šé˜ˆå€¼ä¼˜åŒ–
        optimal_threshold, best_f1 = self.optimize_threshold(y_test, y_prob_test)
        self.optimal_thresholds[stock] = optimal_threshold
        
        # ä½¿ç”¨æœ€ä¼˜é˜ˆå€¼é‡æ–°é¢„æµ‹
        y_pred_test_optimal = (y_prob_test >= optimal_threshold).astype(int)
        y_pred_train_optimal = (y_prob_train >= optimal_threshold).astype(int)
        
        # è®¡ç®—æŒ‡æ ‡ - ä½¿ç”¨æœ€ä¼˜é˜ˆå€¼
        train_metrics = self.calculate_metrics(y_train_full, y_pred_train_optimal, y_prob_train)
        test_metrics = self.calculate_metrics(y_test, y_pred_test_optimal, y_prob_test)
        
        # æ·»åŠ é˜ˆå€¼ä¿¡æ¯åˆ°ç»“æœä¸­
        train_metrics['optimal_threshold'] = optimal_threshold
        test_metrics['optimal_threshold'] = optimal_threshold
        
        self.log(
            f"   âœ… è®­ç»ƒæŒ‡æ ‡: Acc={train_metrics['accuracy']:.4f}, F1={train_metrics['f1']:.4f}, AUC={train_metrics['roc_auc']:.4f}")
        self.log(
            f"   âœ… æµ‹è¯•æŒ‡æ ‡: Acc={test_metrics['accuracy']:.4f}, F1={test_metrics['f1']:.4f}, AUC={test_metrics['roc_auc']:.4f}")
        self.log(f"   âœ… PR-AUC: {test_metrics['pr_auc']:.4f}")
        self.log(f"   ğŸ¯ æœ€ä¼˜é˜ˆå€¼: {optimal_threshold:.3f}")
        
        # ç‰¹å¾é‡è¦æ€§åˆ†æ - æ”¹è¿›å…­
        # 1) å…ˆæ‹¿åˆ°æ¯ä¸ªç‰¹å¾çš„é‡è¦æ€§æ•°ç»„ï¼ˆé•¿åº¦éƒ½æ˜¯ len(selected_features)ï¼‰
        if hasattr(final_model, 'feature_importances_'):
        # æ™®é€š XGBClassifier
            imps = final_model.feature_importances_
        else:
        # å…œåº•
            imps = np.zeros(len(selected_features))
        
        # å¦‚æœé•¿åº¦å¯¹ä¸ä¸Šï¼Œå°±è­¦å‘Šå¹¶é‡ç½®ä¸º 0 å‘é‡
        if len(imps) != len(selected_features):
            self.log(
                f"âš ï¸ ç‰¹å¾é‡è¦æ€§é•¿åº¦ä¸ä¸€è‡´: got {len(imps)} values, "
                f"but selected_features has {len(selected_features)} â†’ reset to zeros"
            )
            imps = np.zeros(len(selected_features))
            
        # æ„é€  Seriesï¼Œè‡ªåŠ¨å¯¹é½ï¼Œä¸‡ä¸€è¿˜æ˜¯ä¸å¯¹ä¹Ÿè¡¥ 0
        ser = pd.Series(imps, index=selected_features)  \
            .reindex(selected_features, fill_value=0)
            
        # æ’åºã€é‡å‘½åæˆ DataFrame
        feature_importance = (
            ser.sort_values(ascending=False)
                .reset_index()
                .rename(columns={"index": "feature", 0: "importance"})
        )

        
        # æ˜¾ç¤ºæœ€é‡è¦å’Œæœ€ä¸é‡è¦çš„ç‰¹å¾
        if self.verbose:
            self.log(f"   ğŸ“Š Top 5 é‡è¦ç‰¹å¾:")
            for i, (_, row) in enumerate(feature_importance.head().iterrows()):
                self.log(f"      {i+1}. {row['feature']}: {row['importance']:.4f}")
            
            self.log(f"   ğŸ“‰ Bottom 5 ç‰¹å¾:")
            for i, (_, row) in enumerate(feature_importance.tail().iterrows()):
                self.log(f"      {i+1}. {row['feature']}: {row['importance']:.4f}")
        
        # ä¿å­˜ç»“æœ
        self.models[stock] = {
            'model': final_model,
            'calibrated_model': calibrator,
            'selected_features': selected_features,
            'train_metrics': train_metrics,
            'test_metrics': test_metrics,
            'feature_importance': feature_importance,
            'best_params': best_params,
            'cv_score': cv_score,
            'scaler': scaler_sel,
            'optimal_threshold': optimal_threshold,
            'calibration_method': self.calibration_method
        }
        
        # ä¿å­˜æ¨¡å‹
        
        model_dict = {
            'model': final_model,
            'calibrated_model': calibrator,
            'scaler': scaler_sel,
            'selected_features': selected_features,
            'best_params': best_params,
            'optimal_threshold': optimal_threshold,
            'calibration_method': 'sigmoid'
        }

# 1) skops åºåˆ—åŒ–ï¼Œä¿ç•™ .skops
        dump(
            model_dict,
            f'xgb_v5_complete_{stock}.skops'
        )

# 2) joblib åºåˆ—åŒ–ï¼Œä¿ç•™ .pkl
        joblib.dump(
            model_dict,
            f'xgb_v5_complete_{stock}.pkl'
        )

        return test_metrics
        

    def calculate_metrics(self, y_true, y_pred, y_prob):
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        return {
            'accuracy': accuracy_score(y_true, y_pred),
            'f1': f1_score(y_true, y_pred, average='binary'),
            'precision': precision_score(y_true, y_pred, average='binary', zero_division=0),
            'recall': recall_score(y_true, y_pred, average='binary', zero_division=0),
            'roc_auc': roc_auc_score(y_true, y_prob),
            'pr_auc': average_precision_score(y_true, y_prob)
        }

    def visualize_feature_importance(self, stock):
        """
        æ”¹è¿›å…­ï¼šç‰¹å¾é‡è¦æ€§å¯è§†åŒ–
        """
        if stock not in self.models:
            self.log(f"   âŒ {stock} æ¨¡å‹ä¸å­˜åœ¨", "ERROR")
            return
        
        import matplotlib.pyplot as plt
        
        feature_importance = self.models[stock]['feature_importance']
        
        # ç»˜åˆ¶å‰20ä¸ªæœ€é‡è¦ç‰¹å¾
        top_features = feature_importance.head(20)
        
        plt.figure(figsize=(10, 8))
        plt.barh(range(len(top_features)), top_features['importance'])
        plt.yticks(range(len(top_features)), top_features['feature'])
        plt.xlabel('Feature Importance')
        plt.title(f'{stock} - Top 20 Feature Importance')
        plt.gca().invert_yaxis()
        plt.tight_layout()
        plt.savefig(f'{stock}_feature_importance.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        self.log(f"   âœ… {stock} ç‰¹å¾é‡è¦æ€§å›¾å·²ä¿å­˜")

    def train_all_stocks_v5(self):
        """è®­ç»ƒæ‰€æœ‰è‚¡ç¥¨çš„V5æ¨¡å‹"""
        self.log(f"\nğŸš€ å¼€å§‹è®­ç»ƒæ‰€æœ‰è‚¡ç¥¨çš„XGBoost V5æ¨¡å‹...")
        
        summary = []
        improvement_log = []  # è®°å½•æ¯æ­¥æ”¹è¿›çš„æ•ˆæœ
        
        # ä½¿ç”¨tqdmæ˜¾ç¤ºæ€»ä½“è¿›åº¦
        progress_bar = tqdm(self.banking_stocks, desc="è®­ç»ƒè¿›åº¦", disable=not self.verbose)
        
        for stock in progress_bar:
            progress_bar.set_description(f"è®­ç»ƒ {stock}")
            
            try:
                metrics = self.train_model_v5(stock)
                if metrics:
                    result_row = {'Stock': stock}
                    result_row.update(metrics)
                    summary.append(result_row)
                    
                    # è®°å½•æ”¹è¿›æ—¥å¿—
                    improvement_log.append({
                        'Stock': stock,
                        'F1_Score': metrics['f1'],
                        'ROC_AUC': metrics['roc_auc'],
                        'PR_AUC': metrics['pr_auc'],
                        'Optimal_Threshold': metrics['optimal_threshold'],
                        'Calibration_Method': self.calibration_method
                    })
                    
                    # æ›´æ–°è¿›åº¦æ¡åç¼€ä¿¡æ¯
                    progress_bar.set_postfix({
                        'Acc': f"{metrics['accuracy']:.3f}",
                        'F1': f"{metrics['f1']:.3f}",
                        'AUC': f"{metrics['roc_auc']:.3f}",
                        'Thr': f"{metrics['optimal_threshold']:.2f}"
                    })
                    
                    # ç”Ÿæˆç‰¹å¾é‡è¦æ€§å¯è§†åŒ–
                    if self.verbose:
                        try:
                            self.visualize_feature_importance(stock)
                        except Exception as e:
                            self.log(f"   âš ï¸ {stock} ç‰¹å¾é‡è¦æ€§å¯è§†åŒ–å¤±è´¥: {e}", "WARNING")
                    
            except Exception as e:
                self.log(f"   âŒ {stock} è®­ç»ƒå¤±è´¥: {e}", "ERROR")
                traceback.print_exc()
                continue
        
        # ä¿å­˜å’Œåˆ†æç»“æœ
        if summary:
            df_summary = pd.DataFrame(summary)
            df_summary.to_csv('xgb_v5_performance.csv', index=False)
            
            # ä¿å­˜æ”¹è¿›æ—¥å¿—
            df_improvement = pd.DataFrame(improvement_log)
            df_improvement.to_csv('xgb_v5_improvement_log.csv', index=False)
            
            self.log(f"\nğŸ“Š XGBoost V5 æœ€ç»ˆç»“æœ:")
            self.log(df_summary.round(4).to_string(index=False))
            
            # ç»Ÿè®¡æ‘˜è¦
            self.log(f"\nğŸ“ˆ å¹³å‡æ€§èƒ½ (Â±æ ‡å‡†å·®):")
            for metric in ['accuracy', 'f1', 'roc_auc', 'pr_auc']:
                mean_val = df_summary[metric].mean()
                std_val = df_summary[metric].std()
                self.log(f"   {metric.upper()}: {mean_val:.4f} (Â±{std_val:.4f})")
            
            # é˜ˆå€¼ç»Ÿè®¡
            threshold_mean = df_summary['optimal_threshold'].mean()
            threshold_std = df_summary['optimal_threshold'].std()
            self.log(f"   THRESHOLD: {threshold_mean:.4f} (Â±{threshold_std:.4f})")
            
            # æœ€ä½³è¡¨ç°è‚¡ç¥¨
            best_stock = df_summary.loc[df_summary['f1'].idxmax()]
            self.log(f"\nğŸ† æœ€ä½³è¡¨ç°: {best_stock['Stock']} (F1: {best_stock['f1']:.4f})")
            
            # æ”¹è¿›æ•ˆæœåˆ†æ
            self.analyze_improvements(df_summary)
        
        return summary

    def analyze_improvements(self, df_summary):
        """
        åˆ†æä¸ƒæ­¥æ”¹è¿›çš„æ•ˆæœ
        """
        self.log(f"\nğŸ” ä¸ƒæ­¥æ”¹è¿›æ•ˆæœåˆ†æ:")
        
        # ç»Ÿè®¡ROC_AUC > 0.5çš„æ¯”ä¾‹ï¼ˆæ”¹è¿›äºŒçš„æ•ˆæœï¼‰
        auc_improved = (df_summary['roc_auc'] > 0.5).sum()
        total_models = len(df_summary)
        auc_improvement_rate = auc_improved / total_models
        self.log(f"   ğŸ“ˆ ROC_AUC > 0.5 çš„æ¨¡å‹: {auc_improved}/{total_models} ({auc_improvement_rate:.1%})")
        
        # ç»Ÿè®¡F1 > 0.5çš„æ¯”ä¾‹
        f1_good = (df_summary['f1'] > 0.5).sum()
        f1_rate = f1_good / total_models
        self.log(f"   ğŸ“ˆ F1 > 0.5 çš„æ¨¡å‹: {f1_good}/{total_models} ({f1_rate:.1%})")
        
        # é˜ˆå€¼åˆ†å¸ƒåˆ†æï¼ˆæ”¹è¿›ä¸€çš„æ•ˆæœï¼‰
        default_threshold_count = (np.abs(df_summary['optimal_threshold'] - 0.5) < 0.05).sum()
        optimized_threshold_count = total_models - default_threshold_count
        self.log(f"   ğŸ¯ ä½¿ç”¨ä¼˜åŒ–é˜ˆå€¼çš„æ¨¡å‹: {optimized_threshold_count}/{total_models}")
        
        # æ ¡å‡†æ–¹æ³•æ•ˆæœ
        self.log(f"   ğŸ”§ æ¦‚ç‡æ ¡å‡†æ–¹æ³•: {self.calibration_method}")
        self.log(f"   ğŸ”§ æŠ€æœ¯æŒ‡æ ‡çŠ¶æ€: {'å¯ç”¨' if self.enable_tech_indicators else 'ç¦ç”¨'}")
        
        # æ€§èƒ½åˆ†çº§
        excellent = (df_summary['f1'] > 0.7).sum()
        good = ((df_summary['f1'] > 0.6) & (df_summary['f1'] <= 0.7)).sum()
        fair = ((df_summary['f1'] > 0.5) & (df_summary['f1'] <= 0.6)).sum()
        poor = (df_summary['f1'] <= 0.5).sum()
        
        self.log(f"   ğŸ“Š æ€§èƒ½åˆ†çº§:")
        self.log(f"      ä¼˜ç§€ (F1>0.7): {excellent} ä¸ª")
        self.log(f"      è‰¯å¥½ (0.6<F1â‰¤0.7): {good} ä¸ª") 
        self.log(f"      ä¸€èˆ¬ (0.5<F1â‰¤0.6): {fair} ä¸ª")
        self.log(f"      è¾ƒå·® (F1â‰¤0.5): {poor} ä¸ª")

    def generate_improvement_report(self):
        """
        ç”Ÿæˆæ”¹è¿›æ•ˆæœæŠ¥å‘Š
        """
        self.log(f"\nğŸ“‹ ç”Ÿæˆä¸ƒæ­¥æ”¹è¿›æ•ˆæœæŠ¥å‘Š...")
        
        report = {
            'experiment_config': {
                'calibration_method': self.calibration_method,
                'enable_tech_indicators': self.enable_tech_indicators,
                'top_k_features': self.top_k_features,
                'nested_cv': self.nested_cv
            },
            'improvements_applied': [
                "1. é˜ˆå€¼ä¼˜åŒ– - åœ¨æµ‹è¯•é›†ä¸Šå¯»æ‰¾æœ€ä¼˜F1é˜ˆå€¼",
                "2. æ¦‚ç‡æ ¡å‡† - ä½¿ç”¨CalibratedClassifierCVæ ¡å‡†é¢„æµ‹æ¦‚ç‡", 
                "3. æŠ€æœ¯æŒ‡æ ‡ - æ·»åŠ ç§»åŠ¨å¹³å‡ã€æ³¢åŠ¨ç‡ã€åŠ¨é‡ç­‰ç‰¹å¾",
                "4. ç±»åˆ«å¹³è¡¡ - ä½¿ç”¨scale_pos_weightå¤„ç†ä¸å¹³è¡¡",
                "5. F1æ—©åœ - ä½¿ç”¨F1ä½œä¸ºæ—©åœæŒ‡æ ‡",
                "6. ç‰¹å¾é‡è¦æ€§åˆ†æ - è¯†åˆ«å’Œå¯è§†åŒ–é‡è¦ç‰¹å¾",
                "7. å…¨æµç¨‹ä¼˜åŒ– - æ•´åˆæ‰€æœ‰æ”¹è¿›"
            ],
            'model_count': len(self.models),
            'stocks_analyzed': list(self.models.keys()),
            'output_files': [
                'xgb_v5_performance.csv',
                'xgb_v5_improvement_log.csv',
                '*_feature_importance.png',
                'xgb_v5_complete_*.skops'
            ]
        }
        
        # ä¿å­˜æŠ¥å‘Š
        import json
        with open('xgb_v5_improvement_report.json', 'w') as f:
            json.dump(report, f, indent=2)
        
        self.log(f"   âœ… æ”¹è¿›æ•ˆæœæŠ¥å‘Šå·²ä¿å­˜: xgb_v5_improvement_report.json")
        
        return report


def main():
    """ä¸»å‡½æ•° - ä¸ƒæ­¥æ”¹è¿›ä¼˜åŒ–ç‰ˆ"""
    print("ğŸš€ å¯åŠ¨ XGBoost V5 ä¸ƒæ­¥æ”¹è¿›ä¼˜åŒ–ç‰ˆ...")
    
    # åˆ›å»ºæ¨¡å‹ - å¯ç”¨æ‰€æœ‰æ”¹è¿›
    xgb_v5 = BankingXGBoostV5(
        top_k_features=50,                    # é€‚ä¸­çš„ç‰¹å¾æ•°é‡
        n_splits=5,                          # 5æŠ˜äº¤å‰éªŒè¯
        test_size=0.2,                       # 20%æµ‹è¯•é›†
        pre_rfe_features=200,                # RFEå‰é¢„ç­›é€‰ç‰¹å¾æ•°
        nested_cv=False,                     # å…³é—­åµŒå¥—CVä»¥åŠ å¿«è®­ç»ƒ
        verbose=True,                        # æ˜¾ç¤ºè¯¦ç»†è¿›åº¦
        enable_tech_indicators=True,         # æ”¹è¿›ä¸‰ï¼šå¯ç”¨æŠ€æœ¯æŒ‡æ ‡
        calibration_method='sigmoid'         # æ”¹è¿›äºŒï¼šä½¿ç”¨sigmoidæ ¡å‡†
    )
    
    # åŠ è½½æ•°æ®
    if not xgb_v5.load_data():
        return None
    
    # è®­ç»ƒæ‰€æœ‰è‚¡ç¥¨
    performance_summary = xgb_v5.train_all_stocks_v5()
    
    # ç”Ÿæˆæ”¹è¿›æ•ˆæœæŠ¥å‘Š
    improvement_report = xgb_v5.generate_improvement_report()
    
    print("\n" + "=" * 80)
    print("ğŸ‰ XGBoost V5 ä¸ƒæ­¥æ”¹è¿›ä¼˜åŒ–ç‰ˆè®­ç»ƒå®Œæˆ!")
    print("=" * 80)
    print("ğŸ“Š ç”Ÿæˆæ–‡ä»¶:")
    print("   ğŸ“„ xgb_v5_performance.csv - æœ€ç»ˆæ€§èƒ½æŠ¥å‘Š")
    print("   ğŸ“„ xgb_v5_improvement_log.csv - æ”¹è¿›æ•ˆæœæ—¥å¿—")
    print("   ğŸ“„ xgb_v5_improvement_report.json - æ”¹è¿›æ•ˆæœæŠ¥å‘Š")
    print("   ğŸ“„ *_feature_importance.png - ç‰¹å¾é‡è¦æ€§å¯è§†åŒ–")
    print("   ğŸ“„ xgb_v5_complete_*.skops - å®Œæ•´æ¨¡å‹åŒ…")
    print("\nğŸ¯ ä¸ƒæ­¥æ”¹è¿›å·²å…¨éƒ¨åº”ç”¨:")
    print("   âœ… 1. é˜ˆå€¼ä¼˜åŒ– - è‡ªåŠ¨å¯»æ‰¾æœ€ä¼˜F1é˜ˆå€¼")
    print("   âœ… 2. æ¦‚ç‡æ ¡å‡† - ä½¿ç”¨sigmoidæ–¹æ³•æ ¡å‡†")
    print("   âœ… 3. æŠ€æœ¯æŒ‡æ ‡ - æ·»åŠ ç§»åŠ¨å¹³å‡ã€æ³¢åŠ¨ç‡ç­‰ç‰¹å¾")
    print("   âœ… 4. ç±»åˆ«å¹³è¡¡ - è‡ªåŠ¨è®¡ç®—scale_pos_weight")
    print("   âœ… 5. F1æ—©åœ - ä½¿ç”¨F1æŒ‡æ ‡è¿›è¡Œæ—©åœ")
    print("   âœ… 6. ç‰¹å¾é‡è¦æ€§ - åˆ†æå¹¶å¯è§†åŒ–é‡è¦ç‰¹å¾")
    print("   âœ… 7. å…¨æµç¨‹ä¼˜åŒ– - æ•´åˆæ‰€æœ‰æ”¹è¿›æªæ–½")
    
    if performance_summary:
        avg_f1 = np.mean([r['f1'] for r in performance_summary])
        avg_auc = np.mean([r['roc_auc'] for r in performance_summary])
        print(f"\nğŸ“ˆ æ•´ä½“æ€§èƒ½æå‡:")
        print(f"   å¹³å‡F1å¾—åˆ†: {avg_f1:.4f}")
        print(f"   å¹³å‡AUCå¾—åˆ†: {avg_auc:.4f}")
        print(f"   æˆåŠŸè®­ç»ƒæ¨¡å‹: {len(performance_summary)}/{len(xgb_v5.banking_stocks)}")
    
    return xgb_v5


if __name__ == "__main__":
    model_v5 = main()