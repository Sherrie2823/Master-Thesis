import os
import glob
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV
from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score,
                             roc_auc_score, average_precision_score)
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, RFE
from sklearn.calibration import CalibratedClassifierCV
from skops.io import dump
from tqdm.auto import tqdm
import warnings

warnings.filterwarnings('ignore')


class BankingRandomForestV4:
    def __init__(self, top_k_features=50, n_splits=5, test_size=0.2,
                 pre_rfe_features=200, nested_cv=True, verbose=True):
        """
        Random Forest V4 - ç»ˆæžä¼˜åŒ–ç‰ˆï¼ˆå·²å¯¹é½ XGB æµç¨‹ï¼‰
        å…³é”®æ”¹åŠ¨ï¼š
        - é»˜è®¤è¯»å– banking_returns_10y.csv
        - è®­ç»ƒåŽä»¥ CalibratedClassifierCV åš Sigmoid æ¦‚çŽ‡æ ¡å‡†
        - ä¿å­˜æ¯åªè‚¡ç¥¨çš„ y_prob_train / y_prob_testï¼ˆå¸¦æ—¥æœŸç´¢å¼•ï¼‰
        - æ–°å¢ž dump_full_probs() å¯¼å‡ºå…¨æœŸæ¦‚çŽ‡ CSVï¼Œä¾›å›žæµ‹è„šæœ¬ç»Ÿä¸€è¯»å–
        """
        self.features = None
        self.targets = None
        self.top_k_features = top_k_features
        self.n_splits = n_splits
        self.test_size = test_size
        self.pre_rfe_features = pre_rfe_features
        self.nested_cv = nested_cv
        self.verbose = verbose

        self.banking_stocks = ['AXP', 'BAC', 'BK', 'C', 'CB', 'COF', 'GS',
                               'JPM', 'MS', 'PNC', 'SCHW', 'STT', 'TFC', 'USB', 'WFC']
        self.models = {}
        self.scalers = {}
        self.feature_selectors = {}
        self.results = {}  # {stock: {'y_prob_train': Series, 'y_prob_test': Series}}

        # ä¸¤é˜¶æ®µè¶…å‚æ•°æœç´¢
        self.coarse_param_grid = {
            'n_estimators': [100, 200, 300],
            'max_depth': [5, 10, 15, 20],
            'min_samples_split': [5, 10, 20],
            'min_samples_leaf': [2, 5, 10],
            'max_features': ['sqrt', 'log2', 0.5],
            'class_weight': ['balanced', 'balanced_subsample']
        }
        self.fine_param_grid = {}

    def log(self, message, level="INFO"):
        if self.verbose:
            print(f"[{level}] {message}")

    # === CHANGED: é»˜è®¤ç‰¹å¾æ–‡ä»¶æ”¹ä¸º banking_returns_10y.csv ===
    def load_data(self, feature_path='banking_returns_10y.csv', target_path='banking_targets_ai.csv'):
        self.log("=" * 80)
        self.log("ðŸŒ² Random Forest V5 - ç»ˆæžä¼˜åŒ–ç‰ˆï¼ˆå·²å¯¹é½ XGB è¾“å‡ºï¼‰")
        self.log("=" * 80)
        self.log("ðŸ“Š 1. åŠ è½½æ•°æ®...")

        self.features = pd.read_csv(feature_path, index_col='Date', parse_dates=True)
        self.targets = pd.read_csv(target_path, index_col='Date', parse_dates=True)

        self.log(f"   âœ… ç‰¹å¾æ•°æ®: {self.features.shape}")
        self.log(f"   âœ… ç›®æ ‡æ•°æ®: {self.targets.shape}")
        self.log(f"   âœ… æ—¶é—´èŒƒå›´: {self.features.index[0]} åˆ° {self.features.index[-1]}")
        self.log(f"   âœ… é…ç½®: Top-{self.top_k_features}ç‰¹å¾, {self.n_splits}æŠ˜CV, é¢„ç­›é€‰{self.pre_rfe_features}ç‰¹å¾, åµŒå¥—CV={'å¼€å¯' if self.nested_cv else 'å…³é—­'}")
        return True

    def prepare_data_no_leakage(self, stock, task_type='direction', horizon='5D'):
        """æ— æ•°æ®æ³„æ¼çš„æ•°æ®å‡†å¤‡ï¼ˆé»˜è®¤ 5D ä¸Ž XGB å¯¹é½ï¼‰"""
        target_col = f'{stock}_Direction_{horizon}'
        if target_col not in self.targets.columns:
            self.log(f"   âš ï¸ ç›®æ ‡å˜é‡ {target_col} ä¸å­˜åœ¨!", "WARNING")
            return None, None

        X = self.features.dropna()
        y = self.targets[target_col].dropna()
        valid_idx = X.index.intersection(y.index)
        X, y = X.loc[valid_idx], y.loc[valid_idx]

        self.log(f"   ðŸ“ˆ {stock}: {X.shape[0]} æ ·æœ¬, {X.shape[1]} ç‰¹å¾")
        self.log(f"   ðŸ“Š ç±»åˆ«åˆ†å¸ƒ: {dict(y.value_counts().sort_index())}")
        return X, y

    def hierarchical_feature_selection(self, X_train, y_train, stock):
        """
        åˆ†å±‚ç‰¹å¾é€‰æ‹©: SelectKBest -> RF Importance -> RFE
        """
        n_samples = len(X_train)
        original_features = X_train.shape[1]

        self.log(f"   ðŸ” å¼€å§‹åˆ†å±‚ç‰¹å¾é€‰æ‹© (åŽŸå§‹ç‰¹å¾: {original_features})")

        # ç¬¬ä¸€å±‚: ç»Ÿè®¡ç­›é€‰ï¼ˆäº’ä¿¡æ¯ï¼Œé€Ÿåº¦å¿«ä¸”éžçº¿æ€§ï¼‰
        if original_features > self.pre_rfe_features:
            selector_stat = SelectKBest(score_func=mutual_info_classif, k=self.pre_rfe_features)
            X_stat = selector_stat.fit_transform(X_train, y_train)
            selected_features_stat = X_train.columns[selector_stat.get_support()].tolist()
            self.log(f"   ðŸ“‰ ç»Ÿè®¡ç­›é€‰: {original_features} -> {len(selected_features_stat)}")
        else:
            X_stat = X_train
            selected_features_stat = X_train.columns.tolist()
            self.log(f"   ðŸ“‰ è·³è¿‡ç»Ÿè®¡ç­›é€‰ (ç‰¹å¾æ•°å·²å°‘äºŽé˜ˆå€¼)")

        # ç¬¬äºŒå±‚: éšæœºæ£®æž—é‡è¦æ€§
        rf_selector = RandomForestClassifier(
            n_estimators=100,
            random_state=42,
            n_jobs=-1,
            class_weight='balanced'
        )
        rf_selector.fit(X_train[selected_features_stat], y_train)

        importances = pd.Series(rf_selector.feature_importances_, index=selected_features_stat)
        rf_top_features = importances.nlargest(min(self.top_k_features * 2, len(selected_features_stat))).index.tolist()
        self.log(f"   ðŸŒ² RFé‡è¦æ€§ç­›é€‰: {len(selected_features_stat)} -> {len(rf_top_features)}")

        # ç¬¬ä¸‰å±‚: RFEï¼ˆæ ·æœ¬æ•°è¶³å¤Ÿæ—¶ï¼‰
        if n_samples >= 500 and len(rf_top_features) > self.top_k_features:
            self.log(f"   ðŸ”„ æ‰§è¡ŒRFE (æ ·æœ¬æ•°å……è¶³: {n_samples})")
            X_rf = X_train[rf_top_features]
            rfe = RFE(
                estimator=RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1),
                n_features_to_select=self.top_k_features,
                step=0.1
            )
            with tqdm(desc="RFEè¿›åº¦", disable=not self.verbose) as pbar:
                rfe.fit(X_rf, y_train)
                pbar.update(1)
            final_features = X_rf.columns[rfe.support_].tolist()
            self.log(f"   ðŸŽ¯ RFEç­›é€‰: {len(rf_top_features)} -> {len(final_features)}")
        else:
            if n_samples < 500:
                self.log(f"   âš ï¸ RFEè·³è¿‡ (æ ·æœ¬æ•°ä¸è¶³: {n_samples})")
            elif len(rf_top_features) <= self.top_k_features:
                self.log(f"   âš ï¸ RFEè·³è¿‡ (ç‰¹å¾æ•°å·²è¾¾æ ‡: {len(rf_top_features)})")
            final_features = rf_top_features[:self.top_k_features]
            self.log(f"   âš ï¸ è·³è¿‡RFE (æ ·æœ¬ä¸è¶³æˆ–ç‰¹å¾å·²è¾¾æ ‡)")

        self.log(f"   âœ… æœ€ç»ˆé€‰æ‹©ç‰¹å¾: {len(final_features)}")
        return final_features

    def nested_cross_validation(self, X_train, y_train):
        outer_scores = []
        best_params_list = []
        outer_cv = TimeSeriesSplit(n_splits=self.n_splits)
        self.log(f"   ðŸ”„ åµŒå¥—CV: {self.n_splits}æŠ˜å¤–å±‚éªŒè¯")
        for fold_idx, (train_idx, val_idx) in enumerate(outer_cv.split(X_train)):
            if self.verbose:
                print(f"      æŠ˜æ•° {fold_idx + 1}/{self.n_splits}")
            X_train_fold = X_train.iloc[train_idx]
            X_val_fold = X_train.iloc[val_idx]
            y_train_fold = y_train.iloc[train_idx]
            y_val_fold = y_train.iloc[val_idx]

            inner_cv = TimeSeriesSplit(n_splits=3)
            model_coarse = RandomForestClassifier(random_state=42, n_jobs=-1)
            search_coarse = RandomizedSearchCV(
                estimator=model_coarse,
                param_distributions=self.coarse_param_grid,
                n_iter=20,
                scoring='f1',
                cv=inner_cv,
                random_state=42,
                n_jobs=-1,
                verbose=0
            )
            search_coarse.fit(X_train_fold, y_train_fold)

            best_coarse = search_coarse.best_params_
            fine_grid = self.generate_fine_grid(best_coarse)

            model_fine = RandomForestClassifier(random_state=42, n_jobs=-1)
            search_fine = RandomizedSearchCV(
                estimator=model_fine,
                param_distributions=fine_grid,
                n_iter=20,
                scoring='f1',
                cv=inner_cv,
                random_state=42,
                n_jobs=-1,
                verbose=0
            )
            search_fine.fit(X_train_fold, y_train_fold)

            best_model = search_fine.best_estimator_
            y_pred_val = best_model.predict(X_val_fold)
            fold_score = f1_score(y_val_fold, y_pred_val)

            outer_scores.append(fold_score)
            best_params_list.append(search_fine.best_params_)
            if self.verbose:
                print(f"         F1: {fold_score:.4f}, å‚æ•°: {search_fine.best_params_}")

        avg_score = np.mean(outer_scores)
        std_score = np.std(outer_scores)
        self.log(f"   âœ… åµŒå¥—CVç»“æžœ: F1 = {avg_score:.4f} (Â±{std_score:.4f})")
        best_params = best_params_list[0] if best_params_list else self.coarse_param_grid
        return best_params, avg_score

    def generate_fine_grid(self, coarse_best):
        fine_grid = {}
        base_n_est = coarse_best.get('n_estimators', 200)
        fine_grid['n_estimators'] = [max(50, base_n_est - 50), base_n_est, base_n_est + 50]
        base_depth = coarse_best.get('max_depth', 10)
        if base_depth is not None:
            fine_grid['max_depth'] = [max(3, base_depth - 2), base_depth, base_depth + 2, None]
        else:
            fine_grid['max_depth'] = [15, 20, None]
        base_split = coarse_best.get('min_samples_split', 10)
        fine_grid['min_samples_split'] = [max(2, base_split - 2), base_split, base_split + 5]
        base_leaf = coarse_best.get('min_samples_leaf', 5)
        fine_grid['min_samples_leaf'] = [max(1, base_leaf - 2), base_leaf, base_leaf + 2]
        fine_grid['max_features'] = [coarse_best.get('max_features', 'sqrt')]
        fine_grid['class_weight'] = [coarse_best.get('class_weight', 'balanced')]
        fine_grid['bootstrap'] = [True, False]
        return fine_grid

    # === CHANGED: horizon é»˜è®¤æ”¹ä¸º 5Dï¼Œä¸Ž XGB ä¸€è‡´ ===
    def train_model_v4(self, stock, task_type='direction', horizon='5D'):
        self.log(f"\nðŸŒ² 2. è®­ç»ƒ {stock} æ¨¡åž‹ ({task_type}-{horizon})...")

        X, y = self.prepare_data_no_leakage(stock, task_type, horizon)
        if X is None:
            return None

        split_idx = int(len(X) * (1 - self.test_size))
        X_train_full, X_test_full = X.iloc[:split_idx], X.iloc[split_idx:]
        y_train_full, y_test = y.iloc[:split_idx], y.iloc[split_idx:]

        self.log(f"   ðŸ“Š è®­ç»ƒé›†: {X_train_full.shape[0]} æ ·æœ¬")
        self.log(f"   ðŸ“Š æµ‹è¯•é›†: {X_test_full.shape[0]} æ ·æœ¬")

        scaler = StandardScaler()
        X_train_scaled = pd.DataFrame(
            scaler.fit_transform(X_train_full),
            columns=X_train_full.columns,
            index=X_train_full.index
        )
        X_test_scaled = pd.DataFrame(
            scaler.transform(X_test_full),
            columns=X_test_full.columns,
            index=X_test_full.index
        )
        self.scalers[stock] = scaler

        selected_features = self.hierarchical_feature_selection(X_train_scaled, y_train_full, stock)
        self.feature_selectors[stock] = selected_features

        X_train = X_train_scaled[selected_features]
        X_test = X_test_scaled[selected_features]

        if self.nested_cv:
            best_params, cv_score = self.nested_cross_validation(X_train, y_train_full)
        else:
            tscv = TimeSeriesSplit(n_splits=self.n_splits)
            model = RandomForestClassifier(random_state=42, n_jobs=-1)
            search = RandomizedSearchCV(
                estimator=model,
                param_distributions=self.coarse_param_grid,
                n_iter=30,
                scoring='f1',
                cv=tscv,
                random_state=42,
                n_jobs=-1,
                verbose=1 if self.verbose else 0
            )
            search.fit(X_train, y_train_full)
            best_params = search.best_params_
            cv_score = search.best_score_

        self.log(f"   âœ… æœ€ä½³å‚æ•°: {best_params}")
        self.log(f"   âœ… CV F1å¾—åˆ†: {cv_score:.4f}")

        # === CHANGED: ä½¿ç”¨æ ¡å‡†åŽçš„æ¨¡åž‹äº§ç”Ÿæ¦‚çŽ‡ ===
        final_model = RandomForestClassifier(**best_params, random_state=42, n_jobs=-1)
        final_model.fit(X_train, y_train_full)

        tscv = TimeSeriesSplit(n_splits=self.n_splits)
        # === FIXED: ä¿®æ”¹ base_estimator ä¸º estimator ===
        calibrated = CalibratedClassifierCV(estimator=final_model, method='sigmoid', cv=tscv)
        calibrated.fit(X_train, y_train_full)

        y_pred_train = calibrated.predict(X_train)
        y_pred_test = calibrated.predict(X_test)
        y_prob_train = calibrated.predict_proba(X_train)[:, 1]
        y_prob_test = calibrated.predict_proba(X_test)[:, 1]

        train_metrics = self.calculate_metrics(y_train_full, y_pred_train, y_prob_train)
        test_metrics = self.calculate_metrics(y_test, y_pred_test, y_prob_test)

        self.log(
            f"   âœ… è®­ç»ƒæŒ‡æ ‡: Acc={train_metrics['accuracy']:.4f}, F1={train_metrics['f1']:.4f}, AUC={train_metrics['roc_auc']:.4f}")
        self.log(
            f"   âœ… æµ‹è¯•æŒ‡æ ‡: Acc={test_metrics['accuracy']:.4f}, F1={test_metrics['f1']:.4f}, AUC={test_metrics['roc_auc']:.4f}")
        self.log(f"   âœ… PR-AUC: {test_metrics['pr_auc']:.4f}")

        feature_importance = pd.DataFrame({
            'feature': selected_features,
            'importance': final_model.feature_importances_
        }).sort_values('importance', ascending=False)

        self.models[stock] = {
            'model': calibrated,  # ä¿å­˜æ ¡å‡†åŽçš„
            'selected_features': selected_features,
            'train_metrics': train_metrics,
            'test_metrics': test_metrics,
            'feature_importance': feature_importance,
            'best_params': best_params,
            'cv_score': cv_score,
            'scaler': scaler
        }

        # === CHANGED: ä¿å­˜è®­ç»ƒ/æµ‹è¯•æ¦‚çŽ‡ï¼ˆå¸¦æ—¥æœŸç´¢å¼•ï¼‰ï¼Œç”¨äºŽå¯¼å‡ºå…¨æœŸæ¦‚çŽ‡ ===
        self.results.setdefault(stock, {})
        self.results[stock]['y_prob_train'] = pd.Series(y_prob_train, index=X_train.index, name=stock)
        self.results[stock]['y_prob_test'] = pd.Series(y_prob_test, index=X_test.index, name=stock)

        # === CHANGED: skops dump ä¿å­˜æ ¡å‡†åŽçš„æ¨¡åž‹ ===
        dump({
            'model': calibrated,
            'scaler': scaler,
            'selected_features': selected_features,
            'best_params': best_params
        }, f'rf_v4_complete_{stock}.skops')
        return test_metrics

    def calculate_metrics(self, y_true, y_pred, y_prob):
        return {
            'accuracy': accuracy_score(y_true, y_pred),
            'f1': f1_score(y_true, y_pred, average='binary'),
            'precision': precision_score(y_true, y_pred, average='binary', zero_division=0),
            'recall': recall_score(y_true, y_pred, average='binary', zero_division=0),
            'roc_auc': roc_auc_score(y_true, y_prob),
            'pr_auc': average_precision_score(y_true, y_prob)
        }

    # === NEW: æ‹¼æŽ¥ & å¯¼å‡ºå…¨æœŸæ¦‚çŽ‡ ===
    def get_full_period_probability(self, stock: str):
        if stock not in self.results:
            return None
        ser_train = self.results[stock].get('y_prob_train')
        ser_test = self.results[stock].get('y_prob_test')
        parts = [s for s in [ser_train, ser_test] if s is not None]
        if not parts:
            return None
        full = pd.concat(parts).sort_index()
        return full.rename(stock)

    def dump_full_probs(self, tickers, horizon='5D', outdir='results'):
        os.makedirs(outdir, exist_ok=True)
        cols = []
        for s in tickers:
            ser = self.get_full_period_probability(s)
            if ser is not None:
                cols.append(ser)
        if not cols:
            self.log('âš ï¸ æ²¡æœ‰å¯å¯¼å‡ºçš„ RF æ¦‚çŽ‡ã€‚')
            return None
        df = pd.concat(cols, axis=1).sort_index()
        ts = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')
        out = os.path.join(outdir, f'rf_probs_{horizon}_{ts}.csv')
        df.to_csv(out, index_label='Date')
        self.log(f'âœ… å¯¼å‡º RF å…¨æœŸæ¦‚çŽ‡ï¼š{out}')
        return out

    # === CHANGED: train_all å¢žåŠ  horizon å‚æ•°ï¼Œå¹¶åœ¨ç»“æŸåŽå¯¼å‡ºå…¨æœŸæ¦‚çŽ‡ ===
    def train_all_stocks_v4(self, task_type='direction', horizon='5D'):
        self.log(f"\nðŸš€ å¼€å§‹è®­ç»ƒæ‰€æœ‰è‚¡ç¥¨çš„V4æ¨¡åž‹... ({task_type}-{horizon})")
        summary = []
        progress_bar = tqdm(self.banking_stocks, desc="è®­ç»ƒè¿›åº¦", disable=not self.verbose)
        for stock in progress_bar:
            progress_bar.set_description(f"è®­ç»ƒ {stock}")
            try:
                metrics = self.train_model_v4(stock, task_type=task_type, horizon=horizon)
                if metrics:
                    row = {'Stock': stock}
                    row.update(metrics)
                    summary.append(row)
                    progress_bar.set_postfix({
                        'Acc': f"{metrics['accuracy']:.3f}",
                        'F1': f"{metrics['f1']:.3f}",
                        'AUC': f"{metrics['roc_auc']:.3f}"
                    })
            except Exception as e:
                self.log(f"   âŒ {stock} è®­ç»ƒå¤±è´¥: {e}", "ERROR")
                continue

        if summary:
            df_summary = pd.DataFrame(summary)
            df_summary.to_csv('rf_v4_performance.csv', index=False)
            self.log(f"\nðŸ“Š Random Forest V4 æœ€ç»ˆç»“æžœ:")
            self.log(df_summary.round(4).to_string(index=False))
            self.log(f"\nðŸ“ˆ å¹³å‡æ€§èƒ½ (Â±æ ‡å‡†å·®):")
            for metric in ['accuracy', 'f1', 'roc_auc', 'pr_auc']:
                mean_val = df_summary[metric].mean()
                std_val = df_summary[metric].std()
                self.log(f"   {metric.upper()}: {mean_val:.4f} (Â±{std_val:.4f})")
            best_stock = df_summary.loc[df_summary['f1'].idxmax()]
            self.log(f"\nðŸ† æœ€ä½³è¡¨çŽ°: {best_stock['Stock']} (F1: {best_stock['f1']:.4f})")

        # å¯¼å‡ºå…¨æœŸæ¦‚çŽ‡ CSVï¼ˆä¸Ž XGB åŒæ¬¾ç»“æž„ï¼‰
        self.dump_full_probs(self.banking_stocks, horizon=horizon, outdir='results')
        return summary


def main():
    print("ðŸš€ å¯åŠ¨ Random Forest V4 ç»ˆæžä¼˜åŒ–ç‰ˆï¼ˆå·²å¯¹é½ XGB è¾“å‡ºï¼‰...")
    rf_v4 = BankingRandomForestV4(
        top_k_features=50,
        n_splits=5,
        test_size=0.2,
        pre_rfe_features=200,
        nested_cv=True,
        verbose=True
    )

    if not rf_v4.load_data():
        return None

    # === CHANGED: é»˜è®¤ horizon=5D ä¸Ž XGB å¯¹é½ ===
    performance_summary = rf_v4.train_all_stocks_v4(task_type='direction', horizon='5D')

    print("\n" + "=" * 80)
    print("ðŸŽ‰ Random Forest V4 ç»ˆæžä¼˜åŒ–ç‰ˆè®­ç»ƒå®Œæˆ!")
    print("=" * 80)
    print("ðŸ“Š ç”Ÿæˆæ–‡ä»¶:")
    print("   ðŸ“„ rf_v4_performance.csv - æœ€ç»ˆæ€§èƒ½æŠ¥å‘Š")
    print("   ðŸ“„ rf_v4_complete_*.skops - å®Œæ•´æ¨¡åž‹åŒ… (æ¨¡åž‹+é¢„å¤„ç†+ç‰¹å¾)")
    print("   ðŸ“„ results/rf_probs_5D_*.csv - å…¨æœŸæ¦‚çŽ‡ï¼ˆä¸Ž XGB å¯¹é½ï¼‰")
    return rf_v4


if __name__ == "__main__":
    _ = main()