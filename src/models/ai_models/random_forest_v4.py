import os
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier

from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV
from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score,
                             roc_auc_score, average_precision_score)
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest, f_classif, RFE
import joblib
from skops.io import dump
from tqdm.auto import tqdm
import warnings

warnings.filterwarnings('ignore')


class BankingRandomForestV4:
    def __init__(self, top_k_features=200, n_splits=5, test_size=0.2,
                 pre_rfe_features=200, nested_cv=True, verbose=True):
        """
        Random Forest V4 - ç»ˆæžä¼˜åŒ–ç‰ˆ

        Parameters:
        - top_k_features: æœ€ç»ˆé€‰æ‹©çš„ç‰¹å¾æ•°é‡
        - n_splits: æ—¶åºäº¤å‰éªŒè¯æŠ˜æ•°
        - test_size: æœ€ç»ˆæµ‹è¯•é›†æ¯”ä¾‹
        - pre_rfe_features: RFEå‰çš„é¢„ç­›é€‰ç‰¹å¾æ•°
        - nested_cv: æ˜¯å¦ä½¿ç”¨åµŒå¥—äº¤å‰éªŒè¯
        - verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†è¿›åº¦
        """
        self.features = None
        self.targets = None
        self.top_k_features = top_k_features
        self.n_splits = n_splits
        self.test_size = test_size
        self.pre_rfe_features = pre_rfe_features
        self.nested_cv = nested_cv
        self.verbose = verbose

        self.banking_stocks = ['AXP', 'BAC', 'BK', 'C', 'CB', 'COF', 'GS',
                               'JPM', 'MS', 'PNC', 'SCHW', 'STT', 'TFC', 'USB', 'WFC']
        self.models = {}
        self.scalers = {}
        self.feature_selectors = {}
        self.results = {}

        # ä¸¤é˜¶æ®µè¶…å‚æ•°æœç´¢
        self.coarse_param_grid = {
            'n_estimators': [100, 200, 300],
            'max_depth': [5, 10, 15, 20],
            'min_samples_split': [5, 10, 20],
            'min_samples_leaf': [2, 5, 10],
            'max_features': ['sqrt', 'log2', 0.5],
            'class_weight': ['balanced', 'balanced_subsample']
        }

        self.fine_param_grid = {}  # åŠ¨æ€ç”Ÿæˆ

    def log(self, message, level="INFO"):
        """æ—¥å¿—è¾“å‡º"""
        if self.verbose:
            print(f"[{level}] {message}")

    def load_data(self, feature_path='banking_returns.csv', target_path='banking_targets_ai.csv'):
        self.log("=" * 80)
        self.log("ðŸŒ² Random Forest V4 - ç»ˆæžä¼˜åŒ–ç‰ˆ")
        self.log("=" * 80)
        self.log("ðŸ“Š 1. åŠ è½½æ•°æ®...")

        self.features = pd.read_csv(feature_path, index_col='Date', parse_dates=True)
        self.targets = pd.read_csv(target_path, index_col='Date', parse_dates=True)

        self.log(f"   âœ… ç‰¹å¾æ•°æ®: {self.features.shape}")
        self.log(f"   âœ… ç›®æ ‡æ•°æ®: {self.targets.shape}")
        self.log(f"   âœ… æ—¶é—´èŒƒå›´: {self.features.index[0]} åˆ° {self.features.index[-1]}")
        self.log(f"   âœ… é…ç½®: Top-{self.top_k_features}ç‰¹å¾, {self.n_splits}æŠ˜CV, " +
                 f"é¢„ç­›é€‰{self.pre_rfe_features}ç‰¹å¾, åµŒå¥—CV={'å¼€å¯' if self.nested_cv else 'å…³é—­'}")
        return True

    def prepare_data_no_leakage(self, stock, task_type='direction', horizon='1D'):
        """æ— æ•°æ®æ³„æ¼çš„æ•°æ®å‡†å¤‡"""
        target_col = f'{stock}_Direction_{horizon}'
        if target_col not in self.targets.columns:
            self.log(f"   âš ï¸ ç›®æ ‡å˜é‡ {target_col} ä¸å­˜åœ¨!", "WARNING")
            return None, None

        X = self.features.dropna()
        y = self.targets[target_col].dropna()
        valid_idx = X.index.intersection(y.index)
        X, y = X.loc[valid_idx], y.loc[valid_idx]

        self.log(f"   ðŸ“ˆ {stock}: {X.shape[0]} æ ·æœ¬, {X.shape[1]} ç‰¹å¾")
        self.log(f"   ðŸ“Š ç±»åˆ«åˆ†å¸ƒ: {dict(y.value_counts().sort_index())}")

        return X, y

    def hierarchical_feature_selection(self, X_train, y_train, stock):
        """
        åˆ†å±‚ç‰¹å¾é€‰æ‹©: SelectKBest -> RF Importance -> RFE
        é¿å…RFEåœ¨å…¨ç‰¹å¾é›†ä¸Šçš„æ€§èƒ½é—®é¢˜
        """
        n_samples = len(X_train)
        original_features = X_train.shape[1]

        self.log(f"   ðŸ” å¼€å§‹åˆ†å±‚ç‰¹å¾é€‰æ‹© (åŽŸå§‹ç‰¹å¾: {original_features})")

        # ç¬¬ä¸€å±‚: ç»Ÿè®¡ç­›é€‰ (å¿«é€Ÿ)
        if original_features > self.pre_rfe_features:
            selector_stat = SelectKBest(score_func=mutual_info_classif, k=self.pre_rfe_features)
            X_stat = selector_stat.fit_transform(X_train, y_train)
            selected_features_stat = X_train.columns[selector_stat.get_support()].tolist()
            self.log(f"   ðŸ“‰ ç»Ÿè®¡ç­›é€‰: {original_features} -> {len(selected_features_stat)}")
        else:
            X_stat = X_train
            selected_features_stat = X_train.columns.tolist()
            self.log(f"   ðŸ“‰ è·³è¿‡ç»Ÿè®¡ç­›é€‰ (ç‰¹å¾æ•°å·²å°‘äºŽé˜ˆå€¼)")

        # ç¬¬äºŒå±‚: éšæœºæ£®æž—é‡è¦æ€§
        rf_selector = RandomForestClassifier(
            n_estimators=100,  # ç”¨è¾ƒå°‘æ ‘åŠ é€Ÿ
            random_state=42,
            n_jobs=-1,
            class_weight='balanced'
        )
        rf_selector.fit(X_stat, y_train)

        # é€‰æ‹©é‡è¦æ€§æœ€é«˜çš„ç‰¹å¾
        importances = pd.Series(rf_selector.feature_importances_, index=selected_features_stat)
        rf_top_features = importances.nlargest(min(self.top_k_features * 2, len(selected_features_stat))).index.tolist()
        self.log(f"   ðŸŒ² RFé‡è¦æ€§ç­›é€‰: {len(selected_features_stat)} -> {len(rf_top_features)}")

        # ç¬¬ä¸‰å±‚: RFE (ä»…åœ¨åˆç†æ ·æœ¬æ•°ä¸‹ä½¿ç”¨)
        if n_samples >= 500 and len(rf_top_features) > self.top_k_features:
            self.log(f"   ðŸ”„ æ‰§è¡ŒRFE (æ ·æœ¬æ•°å……è¶³: {n_samples})")
            X_rf = X_train[rf_top_features]

            rfe = RFE(
                estimator=RandomForestClassifier(
                    n_estimators=50,  # è¿›ä¸€æ­¥å‡å°‘æ ‘çš„æ•°é‡
                    random_state=42,
                    n_jobs=-1
                ),
                n_features_to_select=self.top_k_features,
                step=0.1  # æ¯æ¬¡åˆ é™¤10%çš„ç‰¹å¾
            )

            with tqdm(desc="RFEè¿›åº¦", disable=not self.verbose) as pbar:
                rfe.fit(X_rf, y_train)
                pbar.update(1)

            final_features = X_rf.columns[rfe.support_].tolist()
            self.log(f"   ðŸŽ¯ RFEç­›é€‰: {len(rf_top_features)} -> {len(final_features)}")
        else:
            if n_samples < 500:
                self.log(f"   âš ï¸ RFEè·³è¿‡ (æ ·æœ¬æ•°ä¸è¶³: {n_samples})")
            elif len(rf_top_features) <= self.top_k_features:
                self.log(f"   âš ï¸ RFEè·³è¿‡ (ç‰¹å¾æ•°å·²è¾¾æ ‡: {len(rf_top_features)})")
            final_features = rf_top_features[:self.top_k_features]
            self.log(f"   âš ï¸ è·³è¿‡RFE (æ ·æœ¬ä¸è¶³æˆ–ç‰¹å¾å·²è¾¾æ ‡)")

        self.log(f"   âœ… æœ€ç»ˆé€‰æ‹©ç‰¹å¾: {len(final_features)}")
        return final_features

    def nested_cross_validation(self, X_train, y_train):
        """
        åµŒå¥—äº¤å‰éªŒè¯: å¤–å±‚è¯„ä¼°æ¨¡åž‹æ€§èƒ½ï¼Œå†…å±‚è°ƒä¼˜è¶…å‚æ•°
        """
        outer_scores = []
        best_params_list = []

        # å¤–å±‚æ—¶åºåˆ†å‰²
        outer_cv = TimeSeriesSplit(n_splits=self.n_splits)

        self.log(f"   ðŸ”„ åµŒå¥—CV: {self.n_splits}æŠ˜å¤–å±‚éªŒè¯")

        for fold_idx, (train_idx, val_idx) in enumerate(outer_cv.split(X_train)):
            if self.verbose:
                print(f"      æŠ˜æ•° {fold_idx + 1}/{self.n_splits}")

            # åˆ†å‰²æ•°æ®
            X_train_fold = X_train.iloc[train_idx]
            X_val_fold = X_train.iloc[val_idx]
            y_train_fold = y_train.iloc[train_idx]
            y_val_fold = y_train.iloc[val_idx]

            # å†…å±‚æ—¶åºäº¤å‰éªŒè¯è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜
            inner_cv = TimeSeriesSplit(n_splits=3)  # å†…å±‚ç”¨è¾ƒå°‘æŠ˜æ•°

            # ç²—æœç´¢
            model_coarse = RandomForestClassifier(random_state=42, n_jobs=-1)
            search_coarse = RandomizedSearchCV(
                estimator=model_coarse,
                param_distributions=self.coarse_param_grid,
                n_iter=20,  # ç²—æœç´¢ç”¨è¾ƒå°‘è¿­ä»£
                scoring='f1',
                cv=inner_cv,
                random_state=42,
                n_jobs=-1,
                verbose=0
            )
            search_coarse.fit(X_train_fold, y_train_fold)

            # ç»†æœç´¢ (åŸºäºŽç²—æœç´¢ç»“æžœ)
            best_coarse = search_coarse.best_params_
            fine_grid = self.generate_fine_grid(best_coarse)

            model_fine = RandomForestClassifier(random_state=42, n_jobs=-1)
            search_fine = RandomizedSearchCV(
                estimator=model_fine,
                param_distributions=fine_grid,
                n_iter=20,
                scoring='f1',
                cv=inner_cv,
                random_state=42,
                n_jobs=-1,
                verbose=0
            )
            search_fine.fit(X_train_fold, y_train_fold)

            # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
            best_model = search_fine.best_estimator_
            y_pred_val = best_model.predict(X_val_fold)
            fold_score = f1_score(y_val_fold, y_pred_val)

            outer_scores.append(fold_score)
            best_params_list.append(search_fine.best_params_)

            if self.verbose:
                print(f"         F1: {fold_score:.4f}, å‚æ•°: {search_fine.best_params_}")

        # è¿”å›žå¹³å‡æœ€ä½³å‚æ•°
        avg_score = np.mean(outer_scores)
        std_score = np.std(outer_scores)

        self.log(f"   âœ… åµŒå¥—CVç»“æžœ: F1 = {avg_score:.4f} (Â±{std_score:.4f})")

        # é€‰æ‹©æœ€ä½³å‚æ•° (ç®€åŒ–ï¼šé€‰æ‹©ç¬¬ä¸€ä¸ªï¼Œå®žé™…å¯ä»¥ç”¨æŠ•ç¥¨)
        best_params = best_params_list[0] if best_params_list else self.coarse_param_grid

        return best_params, avg_score

    def generate_fine_grid(self, coarse_best):
        """åŸºäºŽç²—æœç´¢ç»“æžœç”Ÿæˆç²¾ç»†æœç´¢ç½‘æ ¼"""
        fine_grid = {}

        # n_estimators ç»†æœç´¢
        base_n_est = coarse_best.get('n_estimators', 200)
        fine_grid['n_estimators'] = [max(50, base_n_est - 50), base_n_est, base_n_est + 50]

        # max_depth ç»†æœç´¢
        base_depth = coarse_best.get('max_depth', 10)
        if base_depth is not None:
            fine_grid['max_depth'] = [max(3, base_depth - 2), base_depth, base_depth + 2, None]
        else:
            fine_grid['max_depth'] = [15, 20, None]

        # min_samples_split ç»†æœç´¢
        base_split = coarse_best.get('min_samples_split', 10)
        fine_grid['min_samples_split'] = [max(2, base_split - 2), base_split, base_split + 5]

        # min_samples_leaf ç»†æœç´¢
        base_leaf = coarse_best.get('min_samples_leaf', 5)
        fine_grid['min_samples_leaf'] = [max(1, base_leaf - 2), base_leaf, base_leaf + 2]

        # ä¿æŒæœ€ä½³çš„å…¶ä»–å‚æ•°
        fine_grid['max_features'] = [coarse_best.get('max_features', 'sqrt')]
        fine_grid['class_weight'] = [coarse_best.get('class_weight', 'balanced')]
        fine_grid['bootstrap'] = [True, False]

        return fine_grid

    def train_model_v4(self, stock, task_type='direction', horizon='1D'):
        """è®­ç»ƒV4æ¨¡åž‹ - ç»ˆæžä¼˜åŒ–ç‰ˆ"""
        self.log(f"\nðŸŒ² 2. è®­ç»ƒ {stock} æ¨¡åž‹ ({task_type}-{horizon})...")

        # å‡†å¤‡æ•°æ®
        X, y = self.prepare_data_no_leakage(stock, task_type, horizon)
        if X is None:
            return None

        # æœ€ç»ˆæµ‹è¯•é›†åˆ†å‰²
        split_idx = int(len(X) * (1 - self.test_size))
        X_train_full, X_test_full = X.iloc[:split_idx], X.iloc[split_idx:]
        y_train_full, y_test = y.iloc[:split_idx], y.iloc[split_idx:]

        self.log(f"   ðŸ“Š è®­ç»ƒé›†: {X_train_full.shape[0]} æ ·æœ¬")
        self.log(f"   ðŸ“Š æµ‹è¯•é›†: {X_test_full.shape[0]} æ ·æœ¬")

        # æ ‡å‡†åŒ– (åªåœ¨è®­ç»ƒé›†ä¸Šfit)
        scaler = StandardScaler()
        X_train_scaled = pd.DataFrame(
            scaler.fit_transform(X_train_full),
            columns=X_train_full.columns,
            index=X_train_full.index
        )
        X_test_scaled = pd.DataFrame(
            scaler.transform(X_test_full),
            columns=X_test_full.columns,
            index=X_test_full.index
        )
        self.scalers[stock] = scaler

        # åˆ†å±‚ç‰¹å¾é€‰æ‹©
        selected_features = self.hierarchical_feature_selection(X_train_scaled, y_train_full, stock)
        self.feature_selectors[stock] = selected_features

        X_train = X_train_scaled[selected_features]
        X_test = X_test_scaled[selected_features]

        # è¶…å‚æ•°ä¼˜åŒ–
        if self.nested_cv:
            best_params, cv_score = self.nested_cross_validation(X_train, y_train_full)
        else:
            # æ ‡å‡†æ–¹æ³•
            tscv = TimeSeriesSplit(n_splits=self.n_splits)
            model = RandomForestClassifier(random_state=42, n_jobs=-1)
            search = RandomizedSearchCV(
                estimator=model,
                param_distributions=self.coarse_param_grid,
                n_iter=30,
                scoring='f1',
                cv=tscv,
                random_state=42,
                n_jobs=-1,
                verbose=1 if self.verbose else 0
            )
            search.fit(X_train, y_train_full)
            best_params = search.best_params_
            cv_score = search.best_score_

        self.log(f"   âœ… æœ€ä½³å‚æ•°: {best_params}")
        self.log(f"   âœ… CV F1å¾—åˆ†: {cv_score:.4f}")

        # è®­ç»ƒæœ€ç»ˆæ¨¡åž‹
        final_model = RandomForestClassifier(**best_params, random_state=42, n_jobs=-1)
        final_model.fit(X_train, y_train_full)

        # é¢„æµ‹å’Œè¯„ä¼°
        y_pred_train = final_model.predict(X_train)
        y_pred_test = final_model.predict(X_test)
        y_prob_train = final_model.predict_proba(X_train)[:, 1]
        y_prob_test = final_model.predict_proba(X_test)[:, 1]

        # è®¡ç®—æŒ‡æ ‡
        train_metrics = self.calculate_metrics(y_train_full, y_pred_train, y_prob_train)
        test_metrics = self.calculate_metrics(y_test, y_pred_test, y_prob_test)

        self.log(
            f"   âœ… è®­ç»ƒæŒ‡æ ‡: Acc={train_metrics['accuracy']:.4f}, F1={train_metrics['f1']:.4f}, AUC={train_metrics['roc_auc']:.4f}")
        self.log(
            f"   âœ… æµ‹è¯•æŒ‡æ ‡: Acc={test_metrics['accuracy']:.4f}, F1={test_metrics['f1']:.4f}, AUC={test_metrics['roc_auc']:.4f}")
        self.log(f"   âœ… PR-AUC: {test_metrics['pr_auc']:.4f}")

        # ç‰¹å¾é‡è¦æ€§
        feature_importance = pd.DataFrame({
            'feature': selected_features,
            'importance': final_model.feature_importances_
        }).sort_values('importance', ascending=False)

        # ä¿å­˜ç»“æžœ
        self.models[stock] = {
            'model': final_model,
            'selected_features': selected_features,
            'train_metrics': train_metrics,
            'test_metrics': test_metrics,
            'feature_importance': feature_importance,
            'best_params': best_params,
            'cv_score': cv_score,
            'scaler': scaler
        }
        # ä¿å­˜æ¨¡åž‹

        dump({
            'model': final_model,
            'scaler': scaler,
            'selected_features': selected_features,
            'best_params': best_params
        }, f'rf_v4_complete_{stock}.skops')
        return test_metrics

    def calculate_metrics(self, y_true, y_pred, y_prob):
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        return {
            'accuracy': accuracy_score(y_true, y_pred),
            'f1': f1_score(y_true, y_pred, average='binary'),
            'precision': precision_score(y_true, y_pred, average='binary', zero_division=0),
            'recall': recall_score(y_true, y_pred, average='binary', zero_division=0),
            'roc_auc': roc_auc_score(y_true, y_prob),
            'pr_auc': average_precision_score(y_true, y_prob)
        }

    def train_all_stocks_v4(self):
        """è®­ç»ƒæ‰€æœ‰è‚¡ç¥¨çš„V4æ¨¡åž‹"""
        self.log(f"\nðŸš€ å¼€å§‹è®­ç»ƒæ‰€æœ‰è‚¡ç¥¨çš„V4æ¨¡åž‹...")

        summary = []

        # ä½¿ç”¨tqdmæ˜¾ç¤ºæ€»ä½“è¿›åº¦
        progress_bar = tqdm(self.banking_stocks, desc="è®­ç»ƒè¿›åº¦", disable=not self.verbose)

        for stock in progress_bar:
            progress_bar.set_description(f"è®­ç»ƒ {stock}")

            try:
                metrics = self.train_model_v4(stock)
                if metrics:
                    result_row = {'Stock': stock}
                    result_row.update(metrics)
                    summary.append(result_row)

                    # æ›´æ–°è¿›åº¦æ¡åŽç¼€ä¿¡æ¯
                    progress_bar.set_postfix({
                        'Acc': f"{metrics['accuracy']:.3f}",
                        'F1': f"{metrics['f1']:.3f}",
                        'AUC': f"{metrics['roc_auc']:.3f}"
                    })

            except Exception as e:
                self.log(f"   âŒ {stock} è®­ç»ƒå¤±è´¥: {e}", "ERROR")
                continue

        # ä¿å­˜å’Œåˆ†æžç»“æžœ
        if summary:
            df_summary = pd.DataFrame(summary)
            df_summary.to_csv('rf_v4_performance.csv', index=False)

            self.log(f"\nðŸ“Š Random Forest V4 æœ€ç»ˆç»“æžœ:")
            self.log(df_summary.round(4).to_string(index=False))

            # ç»Ÿè®¡æ‘˜è¦
            self.log(f"\nðŸ“ˆ å¹³å‡æ€§èƒ½ (Â±æ ‡å‡†å·®):")
            for metric in ['accuracy', 'f1', 'roc_auc', 'pr_auc']:
                mean_val = df_summary[metric].mean()
                std_val = df_summary[metric].std()
                self.log(f"   {metric.upper()}: {mean_val:.4f} (Â±{std_val:.4f})")

            # ç‰ˆæœ¬å¯¹æ¯”
            self.log(f"\nðŸ”„ ç‰ˆæœ¬å¯¹æ¯”:")
            self.log(f"   V1å¹³å‡å‡†ç¡®çŽ‡: 0.4922")
            self.log(f"   V4å¹³å‡å‡†ç¡®çŽ‡: {df_summary['accuracy'].mean():.4f}")
            improvement = (df_summary['accuracy'].mean() - 0.4922) * 100
            self.log(f"   æ”¹è¿›å¹…åº¦: {improvement:+.2f}ä¸ªç™¾åˆ†ç‚¹")

            # æœ€ä½³è¡¨çŽ°è‚¡ç¥¨
            best_stock = df_summary.loc[df_summary['f1'].idxmax()]
            self.log(f"\nðŸ† æœ€ä½³è¡¨çŽ°: {best_stock['Stock']} (F1: {best_stock['f1']:.4f})")

        return summary


def main():
    """ä¸»å‡½æ•°"""
    print("ðŸš€ å¯åŠ¨ Random Forest V4 ç»ˆæžä¼˜åŒ–ç‰ˆ...")

    # åˆ›å»ºæ¨¡åž‹
    rf_v4 = BankingRandomForestV4(
        top_k_features=50,
        n_splits=5,
        test_size=0.2,
        pre_rfe_features=200,  # RFEå‰é¢„ç­›é€‰ç‰¹å¾æ•°
        nested_cv=True,  # å¯ç”¨åµŒå¥—äº¤å‰éªŒè¯
        verbose=True  # æ˜¾ç¤ºè¯¦ç»†è¿›åº¦
    )

    # åŠ è½½æ•°æ®
    if not rf_v4.load_data():
        return None

    # è®­ç»ƒæ‰€æœ‰è‚¡ç¥¨
    performance_summary = rf_v4.train_all_stocks_v4()

    print("\n" + "=" * 80)
    print("ðŸŽ‰ Random Forest V4 ç»ˆæžä¼˜åŒ–ç‰ˆè®­ç»ƒå®Œæˆ!")
    print("=" * 80)
    print("ðŸ“Š ç”Ÿæˆæ–‡ä»¶:")
    print("   ðŸ“„ rf_v4_performance.csv - æœ€ç»ˆæ€§èƒ½æŠ¥å‘Š")
    print("   ðŸ“„ rf_v4_complete_*.pkl - å®Œæ•´æ¨¡åž‹åŒ… (æ¨¡åž‹+é¢„å¤„ç†+ç‰¹å¾)")

    return rf_v4


if __name__ == "__main__":
    model_v4 = main()